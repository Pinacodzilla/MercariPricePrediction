{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mercari Price Prediction Project \n",
    "##  - Part 3 Data Processing, Machine Learning Modeling, and CNN without Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> **Error Metric:** RMSLE (Root Mean Square Logarithmic Error)\n",
    "<br>**Real world/Business Objectives and Constraints:**\n",
    "<br> 1. Predict the price of an item given its condition, description and other related features.\n",
    "<br> 2. Minimize the difference between predicted and actual price (RMSLE)\n",
    "<br> 3. Try to provide some interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inaba3910/opt/anaconda3/lib/python3.7/site-packages/dask/dataframe/utils.py:14: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import scipy\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# modeling\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge,Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# word embedding\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# tuning and processing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# support\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1482535, 16), (693359, 15))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('df_train.csv')\n",
    "test = pd.read_csv('df_test.csv')\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing \n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "# we are removing the words from the stop words list: 'no', 'nor', 'not'\n",
    "stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no description yet'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing 'item_description' for train set\n",
    "preprocessed_desc_train = []\n",
    "for sentance in train['item_description'].values:\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    sent = re.sub('[0-9]',' ',sent)\n",
    "    sent = re.sub('[^0-9A-Za-z]+',' ', sent)\n",
    "    sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n",
    "    preprocessed_desc_train.append(sent.lower().strip())\n",
    "\n",
    "# after preprocesing\n",
    "preprocessed_desc_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['item_description'] = preprocessed_desc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ava viv blouse'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing 'name' for train set\n",
    "preprocessed_name_train = []\n",
    "for sentance in train['name'].values:\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    sent = re.sub('[0-9]',' ',sent)\n",
    "    sent = re.sub('[^0-9A-Za-z]+',' ', sent)\n",
    "    sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n",
    "    preprocessed_name_train.append(sent.lower().strip())\n",
    "\n",
    "# after preprocesing\n",
    "preprocessed_name_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['name'] = preprocessed_name_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_id             False\n",
       "name                 False\n",
       "item_condition_id    False\n",
       "log_price            False\n",
       "brand_name           False\n",
       "shipping             False\n",
       "item_description     False\n",
       "name_len             False\n",
       "desc_len             False\n",
       "main_cat             False\n",
       "subcat1              False\n",
       "subcat2              False\n",
       "negative             False\n",
       "neutral              False\n",
       "positive             False\n",
       "compound             False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check before split\n",
    "train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing 'item_description' for test set\n",
    "preprocessed_desc_test = []\n",
    "for sentance in test['item_description'].values:\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    sent = re.sub('[0-9]',' ',sent)\n",
    "    sent = re.sub('[^0-9A-Za-z]+',' ', sent)\n",
    "    sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n",
    "    preprocessed_desc_test.append(sent.lower().strip())\n",
    "# after preprocesing\n",
    "preprocessed_desc_test[20000]\n",
    "test['item_description'] = preprocessed_desc_test\n",
    "\n",
    "# preprocessing 'name' for test set\n",
    "preprocessed_name_test = []\n",
    "for sentance in test['name'].values:\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    sent = re.sub('[0-9]',' ',sent)\n",
    "    sent = re.sub('[^0-9A-Za-z]+',' ', sent)\n",
    "    sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n",
    "    preprocessed_name_test.append(sent.lower().strip())\n",
    "\n",
    "# after preprocesing\n",
    "preprocessed_name_test[20000]\n",
    "\n",
    "test['name'] = preprocessed_name_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_id              False\n",
       "name                 False\n",
       "item_condition_id    False\n",
       "brand_name           False\n",
       "shipping             False\n",
       "item_description     False\n",
       "name_len             False\n",
       "desc_len             False\n",
       "main_cat             False\n",
       "subcat1              False\n",
       "subcat2              False\n",
       "negative             False\n",
       "neutral              False\n",
       "positive             False\n",
       "compound             False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into train and test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate target variable and features for training\n",
    "y_train = train['log_price']\n",
    "train.drop(['train_id','log_price'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train \n",
    "y=y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing on train and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot-Encoding on Categorical Features using CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed from previous analysis, there are numbers of categorical variables in this dataset, and before moving into machine learning, we need to convert them into numerical format. Here, I chose to use One-Hot Encoding specifically done with respect to train data to avoid data leakage issue. I did not include test data into it because test data is unseen to us. So, if any category appears new while testing, we will ignore that value while converting to One-Hot encoded form.\n",
    "<br>\n",
    "<br>So, here we have ‘Brand Name’, ‘General Category’, ‘Sub category 1’, ‘Sub category 2’ as columns with categorical values. We will convert these categorical values into one-hot encoded form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning function\n",
    "def clean_cat(cat_values):\n",
    "    '''takes categorical column values as arguments and returns list of cleaned categories'''\n",
    "    \n",
    "    catogories = list(cat_values)\n",
    "\n",
    "    cat_list = []\n",
    "    for i in tqdm(catogories):\n",
    "        i = re.sub('[^A-Za-z0-9]+', ' ', i)\n",
    "        i = i.replace(' ','')\n",
    "        i = i.replace('&','_')\n",
    "        cat_list.append(i.strip())\n",
    "    \n",
    "    return cat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1037774/1037774 [00:02<00:00, 518193.22it/s]\n",
      "100%|██████████| 444761/444761 [00:00<00:00, 519002.27it/s]\n",
      "100%|██████████| 693359/693359 [00:01<00:00, 540885.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# cleaning brand name before using count vectorizer\n",
    "X_train['brand_name'] = clean_cat(X_train['brand_name'].values)\n",
    "X_test['brand_name'] = clean_cat(X_test['brand_name'].values)\n",
    "test['brand_name'] = clean_cat(test['brand_name'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrices after one hot encoding\n",
      "(1037774, 4388) \n",
      " (444761, 4388) \n",
      " (693359, 4388)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding brand name\n",
    "vectorizer = CountVectorizer(lowercase=False, binary=True)\n",
    "xtrain_brand_onehot = vectorizer.fit_transform(X_train['brand_name'])\n",
    "xtest_brand_onehot = vectorizer.transform(X_test['brand_name'])\n",
    "test_brand_onehot = vectorizer.transform(test['brand_name'])\n",
    "\n",
    "print(\"Shape of matrices after one hot encoding\")\n",
    "print(xtrain_brand_onehot.shape, \"\\n\", xtest_brand_onehot.shape, \"\\n\", test_brand_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrices after one hot encoding\n",
      "(1037774, 12) \n",
      " (444761, 12) \n",
      " (693359, 12)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding main_cat\n",
    "vectorizer = CountVectorizer(lowercase=False, binary=True)\n",
    "xtrain_maincat_onehot = vectorizer.fit_transform(X_train['main_cat'].values)\n",
    "xtest_maincat_onehot = vectorizer.transform(X_test['main_cat'].values)\n",
    "test_maincat_onehot = vectorizer.transform(test['main_cat'].values)\n",
    "\n",
    "print(\"Shape of matrices after one hot encoding\")\n",
    "print(xtrain_maincat_onehot.shape, \"\\n\", xtest_maincat_onehot.shape, \"\\n\", test_maincat_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrices after one hot encoding\n",
      "(1037774, 142) \n",
      " (444761, 142) \n",
      " (693359, 142)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding subcat1\n",
    "vectorizer = CountVectorizer(lowercase=False, binary=True)\n",
    "xtrain_subcat1_onehot = vectorizer.fit_transform(X_train['subcat1'].values)\n",
    "xtest_subcat1_onehot = vectorizer.transform(X_test['subcat1'].values)\n",
    "test_subcat1_onehot = vectorizer.transform(test['subcat1'].values)\n",
    "print(\"Shape of matrices after one hot encoding\")\n",
    "print(xtrain_subcat1_onehot.shape, \"\\n\", xtest_subcat1_onehot.shape, \"\\n\", test_subcat1_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrices after one hot encoding\n",
      "(1037774, 951) \n",
      " (444761, 951) \n",
      " (693359, 951)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding subcat2\n",
    "vectorizer = CountVectorizer(lowercase=False, binary=True)\n",
    "xtrain_subcat2_onehot = vectorizer.fit_transform(X_train['subcat2'].values)\n",
    "xtest_subcat2_onehot = vectorizer.transform(X_test['subcat2'].values)\n",
    "test_subcat2_onehot = vectorizer.transform(test['subcat2'].values)\n",
    "\n",
    "print(\"Shape of matrices after one hot encoding\")\n",
    "print(xtrain_subcat2_onehot.shape, \"\\n\", xtest_subcat2_onehot.shape, \"\\n\", test_subcat2_onehot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf Vectorization on Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns 'name' and 'item_description' were encoded into TF-IDF vectors of uni-grams, bi-grams and tri-grams. Note that using 1,2,3-grams together would result in a huge number of words in the dictionary of TF-IDF vectorizer and using all of them would result in very high dimensional vectors. To avoid this, I have limited the number of dimensions to 250k for name and 500k for item_description vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrices after vectorization\n",
      "(1037774, 250000) \n",
      " (444761, 250000) \n",
      " (693359, 250000)\n"
     ]
    }
   ],
   "source": [
    "# tfidf encoding 'name'\n",
    "tfidfvectorizer = TfidfVectorizer(ngram_range=(1, 3), min_df=3, max_features=250000)\n",
    "\n",
    "xtrain_name_tfidf = tfidfvectorizer.fit_transform(X_train['name'].values)\n",
    "xtest_name_tfidf = tfidfvectorizer.transform(X_test['name'].values)\n",
    "test_name_tfidf = tfidfvectorizer.transform(test['name'].values)\n",
    "\n",
    "print(\"Shape of matrices after vectorization\")\n",
    "print(xtrain_name_tfidf.shape, \"\\n\", xtest_name_tfidf.shape, \"\\n\", test_name_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrices after vectorization\n",
      "(1037774, 500000) \n",
      " (444761, 500000) \n",
      " (693359, 500000)\n"
     ]
    }
   ],
   "source": [
    "# tfidf encoding 'item_description'\n",
    "tfidfvectorizer = TfidfVectorizer(ngram_range=(1, 3), min_df=5, max_features=500000)\n",
    "xtrain_desc_tfidf = tfidfvectorizer.fit_transform(X_train['item_description'].values)\n",
    "xtest_desc_tfidf = tfidfvectorizer.transform(X_test['item_description'].values)\n",
    "test_desc_tfidf = tfidfvectorizer.transform(test['item_description'].values)\n",
    "\n",
    "print(\"Shape of matrices after vectorization\")\n",
    "print(xtrain_desc_tfidf.shape, \"\\n\", xtest_desc_tfidf.shape, \"\\n\", test_desc_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary purpose of normalization is to scale numeric data from different columns down to an equivalent scale so that the model doesn’t get skewed due to huge variance in a few columns. I have used min-max normalization here because we need some power to supress the effect of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing following columns:  {'desc_len', 'compound', 'positive', 'negative', 'neutral', 'name_len'}\n"
     ]
    }
   ],
   "source": [
    "# pick up numerical columns\n",
    "cols = set(X_train.columns.values)\n",
    "skip_cols = {'item_condition_id', 'brand_name',\n",
    "  'shipping', 'item_description', 'main_cat',\n",
    "  'subcat1', 'subcat2','name'}\n",
    "\n",
    "cols_to_normalize = cols - skip_cols\n",
    "print(\"Normalizing following columns: \", cols_to_normalize)\n",
    "\n",
    "# define min-max normalization funtion\n",
    "def normalize(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        if (feature_name in cols_to_normalize):\n",
    "            max_value = df[feature_name].max()\n",
    "            min_value = df[feature_name].min()\n",
    "            result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize numerical features\n",
    "xtrain_normalized = normalize(X_train)\n",
    "xtest_normalized = normalize(X_test)\n",
    "test_normalized = normalize(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidate All Features to a Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating and storing all numerical features\n",
    "X_xtrain = xtrain_normalized[list(cols_to_normalize)]\n",
    "X_xtest = xtest_normalized[list(cols_to_normalize)]\n",
    "X_test_x = test_normalized[list(cols_to_normalize)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1037774, 2) (444761, 2) (693359, 2)\n"
     ]
    }
   ],
   "source": [
    "# storing categorical features to sparse matrix\n",
    "X_xtrain_cat = csr_matrix(pd.get_dummies(xtrain_normalized[['item_condition_id', 'shipping']], sparse=True).values)\n",
    "X_xtest_cat = csr_matrix(pd.get_dummies(xtest_normalized[['item_condition_id', 'shipping']], sparse=True).values)\n",
    "X_test_cat = csr_matrix(pd.get_dummies(test_normalized[['item_condition_id', 'shipping']], sparse=True).values)\n",
    "print(X_xtrain_cat.shape, X_xtest_cat.shape, X_test_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1037774, 755495) (444761, 755495) (693359, 755495)\n"
     ]
    }
   ],
   "source": [
    "# stack all categorical and text sparse matrices\n",
    "xtrain_sparse = hstack((xtrain_brand_onehot, xtrain_maincat_onehot, xtrain_subcat1_onehot, xtrain_subcat2_onehot, \\\n",
    "              xtrain_name_tfidf, xtrain_desc_tfidf, X_xtrain_cat)).tocsr()\n",
    "xtest_sparse = hstack((xtest_brand_onehot, xtest_maincat_onehot, xtest_subcat1_onehot, xtest_subcat2_onehot, \\\n",
    "             xtest_name_tfidf, xtest_desc_tfidf, X_xtest_cat)).tocsr()\n",
    "test_sparse = hstack((test_brand_onehot, test_maincat_onehot, test_subcat1_onehot, test_subcat2_onehot, \\\n",
    "                      test_name_tfidf, test_desc_tfidf, X_test_cat)).tocsr()\n",
    "print(xtrain_sparse.shape, xtest_sparse.shape, test_sparse.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (1037774, 755501), CV size: (444761, 755501), Test size: (693359, 755501)\n"
     ]
    }
   ],
   "source": [
    "# stack dense feature matrix with categorical and text vectors\n",
    "X_train = hstack((X_xtrain.values, xtrain_sparse)).tocsr()\n",
    "X_test = hstack((X_xtest.values, xtest_sparse)).tocsr()\n",
    "test_x = hstack((X_test_x.values, test_sparse)).tocsr()\n",
    "print('Train size: {}, CV size: {}, Test size: {}' .format(X_train.shape, X_test.shape, test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete variables we do not need further\n",
    "del vectorizer\n",
    "del xtrain_normalized, xtest_normalized, test_normalized\n",
    "del xtrain_brand_onehot, xtrain_maincat_onehot, xtrain_subcat1_onehot, xtrain_subcat2_onehot, X_xtrain_cat,xtrain_name_tfidf, xtrain_desc_tfidf\n",
    "\n",
    "del xtest_brand_onehot, xtest_maincat_onehot, xtest_subcat1_onehot, xtest_subcat2_onehot, X_xtest_cat, xtest_name_tfidf, xtest_desc_tfidf\n",
    "\n",
    "del test_brand_onehot, test_maincat_onehot, test_subcat1_onehot, test_subcat2_onehot, X_test_cat, test_name_tfidf, test_desc_tfidf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Error Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y, y_pred): # return Rmsle value.\n",
    "    return np.sqrt(np.mean(np.square(y_pred - y )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Modeling Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I chose to use ridge regression, lasso regression, light gradient boosting regressor, and xgboost regressor with randomized search for hyperparameter tuning as 4 of the machine learning models. These were chosen particularly because we need fast and robust regression model towards price, and we need a model with a high performance so some models like random forest that is known better performed in categorization problems are excluded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=0.0001 ..................[CV] solver=lsqr, fit_intercept=False, alpha=0.0001 ..................\n",
      "\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=0.0001 ..................\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=0.001 ...................\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=0.001 ...................\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=0.001 ...................\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=0.01 ....................[CV] solver=lsqr, fit_intercept=False, alpha=0.01 ....................\n",
      "\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=0.001, score=(train=-0.198, test=-0.223), total=23.3min[CV]  solver=lsqr, fit_intercept=False, alpha=0.0001, score=(train=-0.198, test=-0.223), total=23.3min\n",
      "\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=0.01 ....................[CV] solver=lsqr, fit_intercept=False, alpha=0.1 .....................\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 23.6min\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed: 23.6min\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=0.01, score=(train=-0.195, test=-0.225), total=23.8min[CV]  solver=lsqr, fit_intercept=False, alpha=0.001, score=(train=-0.195, test=-0.223), total=23.8min[CV]  solver=lsqr, fit_intercept=False, alpha=0.01, score=(train=-0.195, test=-0.223), total=23.8min\n",
      "\n",
      "\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=0.1 .....................\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=0.1 .....................[CV] solver=lsqr, fit_intercept=False, alpha=1 .......................\n",
      "\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=0.0001, score=(train=-0.195, test=-0.225), total=23.8min\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed: 24.2min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=1 .......................[CV]  solver=lsqr, fit_intercept=False, alpha=0.0001, score=(train=-0.195, test=-0.223), total=23.8min\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed: 24.2min\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed: 24.2min\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed: 24.2min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=1 .......................[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed: 24.2min\n",
      "\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=0.001, score=(train=-0.195, test=-0.225), total=23.9min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=10 ......................[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed: 24.3min\n",
      "\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=1, score=(train=-0.198, test=-0.223), total= 6.9min\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed: 31.8min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=10 ......................\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=10, score=(train=-0.212, test=-0.232), total= 5.1min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed: 37.0min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=10 ......................\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=10, score=(train=-0.213, test=-0.228), total= 5.2min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=100 .....................\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed: 42.4min\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=10, score=(train=-0.214, test=-0.231), total=18.4min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=100 .....................[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed: 44.9min\n",
      "\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=100, score=(train=-0.258, test=-0.263), total= 3.0min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=100 .....................[Parallel(n_jobs=-1)]: Done  13 out of  27 | elapsed: 45.9min remaining: 49.4min\n",
      "\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=0.01, score=(train=-0.198, test=-0.223), total=24.2min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=1000 ....................[Parallel(n_jobs=-1)]: Done  14 out of  27 | elapsed: 49.9min remaining: 46.3min\n",
      "\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=0.1, score=(train=-0.195, test=-0.223), total=24.9min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  27 | elapsed: 50.0min remaining: 40.0min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=1000 ....................\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=0.1, score=(train=-0.198, test=-0.223), total=24.0min\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  27 | elapsed: 50.3min remaining: 34.6min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=1000 ....................\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=0.1, score=(train=-0.195, test=-0.225), total=24.2min\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  27 | elapsed: 50.4min remaining: 29.6min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=10000 ...................\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=1, score=(train=-0.195, test=-0.225), total=24.2min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  27 | elapsed: 50.4min remaining: 25.2min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=10000 ...................\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=1, score=(train=-0.195, test=-0.222), total=25.0min\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  27 | elapsed: 52.0min remaining: 21.9min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=10000 ...................\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=1000, score=(train=-0.325, test=-0.330), total= 2.2min\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  27 | elapsed: 52.8min remaining: 18.5min\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=10000, score=(train=-0.448, test=-0.446), total= 1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  27 | elapsed: 53.6min remaining: 15.3min\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=10000, score=(train=-0.447, test=-0.451), total= 4.1min\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  27 | elapsed: 55.2min remaining: 12.5min\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=10000, score=(train=-0.448, test=-0.447), total= 3.5min\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  27 | elapsed: 55.3min remaining:  9.6min\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=1000, score=(train=-0.327, test=-0.326), total= 5.2min\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  27 | elapsed: 55.9min remaining:  7.0min\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=1000, score=(train=-0.326, test=-0.326), total= 5.7min\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  27 | elapsed: 56.1min remaining:  4.5min\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=100, score=(train=-0.256, test=-0.265), total=10.9min\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=100, score=(train=-0.259, test=-0.262), total= 9.9min\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed: 56.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed: 56.2min finished\n"
     ]
    }
   ],
   "source": [
    "# prepare lasso regression and use randomized search for hyperparameter tuning\n",
    "# here, use parallel_backend to boost speed and avoid no end tuning process\n",
    "from sklearn.externals.joblib import parallel_backend\n",
    "parameters = {'alpha':[0.0001,0.001,0.01,0.1,1,10,100,1000,10000],\n",
    "              'fit_intercept' : [False],\n",
    "              'solver' : ['lsqr']}\n",
    "model = Ridge()\n",
    "rs_ridge = RandomizedSearchCV(model,\n",
    "                        param_distributions = parameters,\n",
    "                        cv = 3, \n",
    "                        scoring = 'neg_mean_squared_error',\n",
    "                        verbose = 100,\n",
    "                        return_train_score = True,\n",
    "                        n_iter=10, \n",
    "                        random_state=42)\n",
    "\n",
    "with parallel_backend('threading'):\n",
    "    rs_ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by random search are: {'solver': 'lsqr', 'fit_intercept': False, 'alpha': 1}\n",
      "Best score found by random search is: -0.2232724389821035\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters found by random search are:', rs_ridge.best_params_)\n",
    "print('Best score found by random search is:', rs_ridge.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and get rmsle\n",
    "ridgeReg = Ridge(alpha=1,solver = \"lsqr\", fit_intercept=False )\n",
    "ridgeReg.fit(X_train, y_train)\n",
    "y_pred_ridge = ridgeReg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fold %02d Ridge RMSLE:  0.48015452226677147\n"
     ]
    }
   ],
   "source": [
    "ridge_RMSLE = rmsle(y_test, y_pred_ridge)\n",
    "print(' Fold %02d Ridge RMSLE: ', ridge_RMSLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[CV] fit_intercept=False, alpha=0.0001 ...............................[CV] fit_intercept=False, alpha=0.0001 ...............................\n",
      "[CV] fit_intercept=False, alpha=0.0001 ...............................\n",
      "\n",
      "[CV] fit_intercept=False, alpha=0.001 ................................[CV] fit_intercept=False, alpha=0.001 ................................\n",
      "[CV] fit_intercept=False, alpha=0.001 ................................\n",
      "[CV] fit_intercept=False, alpha=0.01 .................................\n",
      "[CV] fit_intercept=False, alpha=0.01 .................................\n",
      "\n",
      "[CV]  fit_intercept=False, alpha=0.01, score=(train=-0.505, test=-0.505), total= 2.9min\n",
      "[CV]  fit_intercept=False, alpha=0.01, score=(train=-0.505, test=-0.508), total= 2.9min\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  3.1min\n",
      "[CV] fit_intercept=False, alpha=0.1 ..................................[CV] fit_intercept=False, alpha=0.01 .................................\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  3.1min\n",
      "[CV]  fit_intercept=False, alpha=0.1, score=(train=-0.791, test=-0.788), total=  44.5s\n",
      "[CV] fit_intercept=False, alpha=0.1 ..................................\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:  3.9min\n",
      "[CV]  fit_intercept=False, alpha=0.001, score=(train=-0.392, test=-0.389), total= 4.1min\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:  4.3min\n",
      "[CV] fit_intercept=False, alpha=0.1 ..................................\n",
      "[CV]  fit_intercept=False, alpha=0.001, score=(train=-0.389, test=-0.393), total= 4.1min\n",
      "[CV] fit_intercept=False, alpha=1 ....................................\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  4.3min\n",
      "[CV]  fit_intercept=False, alpha=1, score=(train=-2.416, test=-2.409), total=  25.9s\n",
      "[CV] fit_intercept=False, alpha=1 ....................................[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:  4.8min\n",
      "\n",
      "[CV]  fit_intercept=False, alpha=0.1, score=(train=-0.788, test=-0.794), total=  52.1s\n",
      "[CV] fit_intercept=False, alpha=1 ....................................\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:  4.9min\n",
      "[CV]  fit_intercept=False, alpha=0.1, score=(train=-0.792, test=-0.789), total=  38.0s\n",
      "[CV] fit_intercept=False, alpha=10 ...................................[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:  5.0min\n",
      "\n",
      "[CV]  fit_intercept=False, alpha=0.01, score=(train=-0.506, test=-0.504), total= 1.9min\n",
      "[CV] fit_intercept=False, alpha=10 ...................................\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  5.1min\n",
      "[CV]  fit_intercept=False, alpha=1, score=(train=-2.413, test=-2.417), total=  27.5s\n",
      "[CV] fit_intercept=False, alpha=10 ...................................\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  5.4min\n",
      "[CV]  fit_intercept=False, alpha=1, score=(train=-2.412, test=-2.414), total=  21.6s\n",
      "[CV] fit_intercept=False, alpha=100 ..................................[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:  5.4min\n",
      "\n",
      "[CV]  fit_intercept=False, alpha=0.001, score=(train=-0.391, test=-0.391), total= 5.4min\n",
      "[CV] fit_intercept=False, alpha=100 ..................................\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:  5.8min\n",
      "[CV]  fit_intercept=False, alpha=10, score=(train=-9.434, test=-9.428), total=  54.3s\n",
      "[CV] fit_intercept=False, alpha=100 ..................................[Parallel(n_jobs=-1)]: Done  13 out of  27 | elapsed:  6.0min remaining:  6.5min\n",
      "\n",
      "[CV]  fit_intercept=False, alpha=10, score=(train=-9.427, test=-9.442), total=  50.2s\n",
      "[CV] fit_intercept=False, alpha=1000 .................................[Parallel(n_jobs=-1)]: Done  14 out of  27 | elapsed:  6.1min remaining:  5.7min\n",
      "\n",
      "[CV]  fit_intercept=False, alpha=10, score=(train=-9.435, test=-9.426), total=  42.7s\n",
      "[CV] fit_intercept=False, alpha=1000 .................................[Parallel(n_jobs=-1)]: Done  15 out of  27 | elapsed:  6.2min remaining:  5.0min\n",
      "\n",
      "[CV]  fit_intercept=False, alpha=100, score=(train=-9.434, test=-9.428), total=  43.8s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  27 | elapsed:  6.2min remaining:  4.3min\n",
      "[CV] fit_intercept=False, alpha=1000 .................................\n",
      "[CV]  fit_intercept=False, alpha=100, score=(train=-9.427, test=-9.442), total=  55.4s\n",
      "[CV] fit_intercept=False, alpha=10000 ................................[Parallel(n_jobs=-1)]: Done  17 out of  27 | elapsed:  6.9min remaining:  4.1min\n",
      "\n",
      "[CV]  fit_intercept=False, alpha=1000, score=(train=-9.434, test=-9.428), total=  57.4s\n",
      "[CV] fit_intercept=False, alpha=10000 ................................[Parallel(n_jobs=-1)]: Done  18 out of  27 | elapsed:  7.2min remaining:  3.6min\n",
      "\n",
      "[CV]  fit_intercept=False, alpha=100, score=(train=-9.435, test=-9.426), total= 1.1min\n",
      "[CV] fit_intercept=False, alpha=10000 ................................[Parallel(n_jobs=-1)]: Done  19 out of  27 | elapsed:  7.2min remaining:  3.0min\n",
      "\n",
      "[CV]  fit_intercept=False, alpha=1000, score=(train=-9.427, test=-9.442), total=  56.3s\n",
      "[CV]  fit_intercept=False, alpha=1000, score=(train=-9.435, test=-9.426), total=  56.6s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  27 | elapsed:  7.2min remaining:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  27 | elapsed:  7.2min remaining:  2.1min\n",
      "[CV]  fit_intercept=False, alpha=10000, score=(train=-9.434, test=-9.428), total=  24.1s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  27 | elapsed:  7.4min remaining:  1.7min\n",
      "[CV]  fit_intercept=False, alpha=10000, score=(train=-9.435, test=-9.426), total=  11.9s\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  27 | elapsed:  7.4min remaining:  1.3min\n",
      "[CV]  fit_intercept=False, alpha=10000, score=(train=-9.427, test=-9.442), total=  14.3s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  27 | elapsed:  7.4min remaining:   55.6s\n",
      "[CV]  fit_intercept=False, alpha=0.0001, score=(train=-0.308, test=-0.306), total=10.5min\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  27 | elapsed: 10.6min remaining:   50.7s\n",
      "[CV]  fit_intercept=False, alpha=0.0001, score=(train=-0.307, test=-0.307), total=10.6min\n",
      "[CV]  fit_intercept=False, alpha=0.0001, score=(train=-0.306, test=-0.310), total=10.8min\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed: 10.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed: 10.8min finished\n"
     ]
    }
   ],
   "source": [
    "# prepare lasso regression and use randomized search for hyperparameter tuning\n",
    "# here, use parallel_backend to boost speed and avoid no end tuning process\n",
    "from sklearn.externals.joblib import parallel_backend\n",
    "parameters = {'alpha':[0.0001,0.001,0.01,0.1,1,10,100,1000,10000],\n",
    "              'fit_intercept' : [False]}\n",
    "model = Lasso()\n",
    "rs_lasso = RandomizedSearchCV(model,\n",
    "                        param_distributions = parameters,\n",
    "                        cv = 3, \n",
    "                        scoring = 'neg_mean_squared_error',\n",
    "                        verbose = 100,\n",
    "                        return_train_score = True,\n",
    "                        n_iter=10, \n",
    "                        random_state=42)\n",
    "\n",
    "with parallel_backend('threading'):\n",
    "    rs_lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by random search are: {'fit_intercept': False, 'alpha': 0.0001}\n",
      "Best score found by random search is: -0.3074479111859845\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters found by random search are:', rs_lasso.best_params_)\n",
    "print('Best score found by random search is:', rs_lasso.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and get rmsle\n",
    "lassoReg = Lasso(alpha=0.0001, fit_intercept=False )\n",
    "lassoReg.fit(X_train, y_train)\n",
    "y_pred_lasso = lassoReg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fold %02d Lasso RMSLE:  0.556302994133279\n"
     ]
    }
   ],
   "source": [
    "lasso_RMSLE = rmsle(y_test, y_pred_lasso)\n",
    "print(' Fold %02d Lasso RMSLE: ', lasso_RMSLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light GBM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] learning_rate=0.18727005942368125, max_depth=6, n_estimators=470, num_leaves=91 [CV] learning_rate=0.18727005942368125, max_depth=6, n_estimators=470, num_leaves=91 \n",
      "\n",
      "[CV] learning_rate=0.18727005942368125, max_depth=6, n_estimators=470, num_leaves=91 [CV] learning_rate=0.2993292420985183, max_depth=3, n_estimators=414, num_leaves=94 \n",
      "[CV] learning_rate=0.2993292420985183, max_depth=3, n_estimators=414, num_leaves=94 \n",
      "\n",
      "[CV] learning_rate=0.2993292420985183, max_depth=3, n_estimators=414, num_leaves=94 [CV] learning_rate=0.22962444598293358, max_depth=6, n_estimators=299, num_leaves=43 \n",
      "\n",
      "[CV] learning_rate=0.22962444598293358, max_depth=6, n_estimators=299, num_leaves=43 \n",
      "[CV]  learning_rate=0.2993292420985183, max_depth=3, n_estimators=414, num_leaves=94, score=(train=-0.266, test=-0.273), total= 9.2min\n",
      "[CV]  learning_rate=0.2993292420985183, max_depth=3, n_estimators=414, num_leaves=94, score=(train=-0.268, test=-0.270), total= 9.2min[CV]  learning_rate=0.2993292420985183, max_depth=3, n_estimators=414, num_leaves=94, score=(train=-0.268, test=-0.269), total= 9.3min\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed: 10.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] learning_rate=0.22962444598293358, max_depth=6, n_estimators=299, num_leaves=43 \n",
      "[CV] learning_rate=0.32544423647442644, max_depth=6, n_estimators=457, num_leaves=49 \n",
      "[CV] learning_rate=0.32544423647442644, max_depth=6, n_estimators=457, num_leaves=49 \n",
      "[CV]  learning_rate=0.22962444598293358, max_depth=6, n_estimators=299, num_leaves=43, score=(train=-0.249, test=-0.260), total=26.5min\n",
      "[CV] learning_rate=0.32544423647442644, max_depth=6, n_estimators=457, num_leaves=49 \n",
      "[CV]  learning_rate=0.22962444598293358, max_depth=6, n_estimators=299, num_leaves=43, score=(train=-0.250, test=-0.258), total=27.0min\n",
      "[CV] learning_rate=0.10616955533913808, max_depth=5, n_estimators=476, num_leaves=52 \n",
      "[CV]  learning_rate=0.18727005942368125, max_depth=6, n_estimators=470, num_leaves=91, score=(train=-0.242, test=-0.250), total=39.2min\n",
      "[CV]  learning_rate=0.18727005942368125, max_depth=6, n_estimators=470, num_leaves=91, score=(train=-0.240, test=-0.253), total=39.2min\n",
      "[CV] learning_rate=0.10616955533913808, max_depth=5, n_estimators=476, num_leaves=52 \n",
      "[CV]  learning_rate=0.18727005942368125, max_depth=6, n_estimators=470, num_leaves=91, score=(train=-0.243, test=-0.252), total=39.2min\n",
      "[CV] learning_rate=0.10616955533913808, max_depth=5, n_estimators=476, num_leaves=52 \n",
      "[CV] learning_rate=0.15212112147976886, max_depth=7, n_estimators=452, num_leaves=68 \n",
      "[CV]  learning_rate=0.22962444598293358, max_depth=6, n_estimators=299, num_leaves=43, score=(train=-0.251, test=-0.258), total=33.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed: 45.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] learning_rate=0.15212112147976886, max_depth=7, n_estimators=452, num_leaves=68 \n",
      "[CV]  learning_rate=0.32544423647442644, max_depth=6, n_estimators=457, num_leaves=49, score=(train=-0.225, test=-0.238), total=40.9min\n",
      "[CV]  learning_rate=0.32544423647442644, max_depth=6, n_estimators=457, num_leaves=49, score=(train=-0.224, test=-0.242), total=41.0min\n",
      "[CV] learning_rate=0.15212112147976886, max_depth=7, n_estimators=452, num_leaves=68 \n",
      "[CV] learning_rate=0.2623873301291946, max_depth=3, n_estimators=387, num_leaves=99 \n",
      "[CV]  learning_rate=0.10616955533913808, max_depth=5, n_estimators=476, num_leaves=52, score=(train=-0.271, test=-0.275), total=34.0min\n",
      "[CV] learning_rate=0.2623873301291946, max_depth=3, n_estimators=387, num_leaves=99 \n",
      "[CV]  learning_rate=0.2623873301291946, max_depth=3, n_estimators=387, num_leaves=99, score=(train=-0.275, test=-0.277), total=12.6min\n",
      "[CV] learning_rate=0.2623873301291946, max_depth=3, n_estimators=387, num_leaves=99 \n",
      "[CV]  learning_rate=0.32544423647442644, max_depth=6, n_estimators=457, num_leaves=49, score=(train=-0.227, test=-0.239), total=39.8min\n",
      "[CV] learning_rate=0.11638567021515211, max_depth=7, n_estimators=374, num_leaves=81 \n",
      "[CV]  learning_rate=0.10616955533913808, max_depth=5, n_estimators=476, num_leaves=52, score=(train=-0.273, test=-0.275), total=31.6min\n",
      "[CV] learning_rate=0.11638567021515211, max_depth=7, n_estimators=374, num_leaves=81 \n",
      "[CV]  learning_rate=0.10616955533913808, max_depth=5, n_estimators=476, num_leaves=52, score=(train=-0.271, test=-0.278), total=32.0min\n",
      "[CV] learning_rate=0.11638567021515211, max_depth=7, n_estimators=374, num_leaves=81 \n",
      "[CV]  learning_rate=0.2623873301291946, max_depth=3, n_estimators=387, num_leaves=99, score=(train=-0.274, test=-0.281), total=13.6min\n",
      "[CV] learning_rate=0.09983689107917987, max_depth=5, n_estimators=330, num_leaves=70 \n",
      "[CV]  learning_rate=0.2623873301291946, max_depth=3, n_estimators=387, num_leaves=99, score=(train=-0.276, test=-0.276), total=13.7min\n",
      "[CV] learning_rate=0.09983689107917987, max_depth=5, n_estimators=330, num_leaves=70 \n",
      "[CV]  learning_rate=0.15212112147976886, max_depth=7, n_estimators=452, num_leaves=68, score=(train=-0.241, test=-0.251), total=57.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  19 out of  30 | elapsed: 101.8min remaining: 59.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] learning_rate=0.09983689107917987, max_depth=5, n_estimators=330, num_leaves=70 \n",
      "[CV]  learning_rate=0.15212112147976886, max_depth=7, n_estimators=452, num_leaves=68, score=(train=-0.239, test=-0.253), total=59.0min\n",
      "[CV] learning_rate=0.34015376929388985, max_depth=2, n_estimators=366, num_leaves=37 \n",
      "[CV]  learning_rate=0.09983689107917987, max_depth=5, n_estimators=330, num_leaves=70, score=(train=-0.288, test=-0.291), total=30.9min\n",
      "[CV] learning_rate=0.34015376929388985, max_depth=2, n_estimators=366, num_leaves=37 \n",
      "[CV]  learning_rate=0.15212112147976886, max_depth=7, n_estimators=452, num_leaves=68, score=(train=-0.241, test=-0.250), total=57.5min\n",
      "[CV] learning_rate=0.34015376929388985, max_depth=2, n_estimators=366, num_leaves=37 \n",
      "[CV]  learning_rate=0.09983689107917987, max_depth=5, n_estimators=330, num_leaves=70, score=(train=-0.287, test=-0.294), total=31.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  23 out of  30 | elapsed: 115.6min remaining: 35.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.34015376929388985, max_depth=2, n_estimators=366, num_leaves=37, score=(train=-0.289, test=-0.290), total=10.5min\n",
      "[CV]  learning_rate=0.34015376929388985, max_depth=2, n_estimators=366, num_leaves=37, score=(train=-0.286, test=-0.292), total= 7.2min\n",
      "[CV]  learning_rate=0.11638567021515211, max_depth=7, n_estimators=374, num_leaves=81, score=(train=-0.257, test=-0.264), total=48.6min\n",
      "[CV]  learning_rate=0.34015376929388985, max_depth=2, n_estimators=366, num_leaves=37, score=(train=-0.288, test=-0.288), total= 5.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  27 out of  30 | elapsed: 120.0min remaining: 13.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.09983689107917987, max_depth=5, n_estimators=330, num_leaves=70, score=(train=-0.289, test=-0.290), total=18.7min\n",
      "[CV]  learning_rate=0.11638567021515211, max_depth=7, n_estimators=374, num_leaves=81, score=(train=-0.258, test=-0.263), total=44.3min\n",
      "[CV]  learning_rate=0.11638567021515211, max_depth=7, n_estimators=374, num_leaves=81, score=(train=-0.255, test=-0.267), total=44.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 121.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by random search are: {'learning_rate': 0.32544423647442644, 'max_depth': 6, 'n_estimators': 457, 'num_leaves': 49}\n",
      "Best score found by random search is: -0.23973342852724522\n"
     ]
    }
   ],
   "source": [
    "# prepare lightGBM regressor and use randomized search for hyperparameter tuning\n",
    "# instead of grid search, randmized search is more effective and time saving in my models\n",
    "# here, use parallel_backend to boost speed and avoid no end tuning process\n",
    "rsParams = {'learning_rate': uniform(0, 0.5),\n",
    "          'n_estimators': sp_randint(200, 500),\n",
    "          'num_leaves': sp_randint(20, 100),\n",
    "          'max_depth': sp_randint(2, 8)\n",
    "           }\n",
    "lgbm_params ={'subsample': 0.9, 'colsample_bytree': 0.8, 'min_child_samples': 50, 'objective': 'regression'\n",
    "        }\n",
    "model = LGBMRegressor(**lgbm_params)\n",
    "\n",
    "rs_lgbm = RandomizedSearchCV(model, param_distributions=rsParams, n_iter=10, cv=3, random_state=42, \n",
    "                                scoring='neg_mean_squared_error', verbose=10, return_train_score=True)\n",
    "\n",
    "with parallel_backend('threading'):\n",
    "    rs_lgbm.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameters found by random search are:', rs_lgbm.best_params_)\n",
    "print('Best score found by random search is:', rs_lgbm.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and get rmsle\n",
    "lgbm_params ={'subsample': 0.9, 'colsample_bytree': 0.8, 'min_child_samples': 50, 'objective': 'regression','boosting_type': 'gbdt','learning_rate': 0.32544423647442644,'max_depth': 6,'n_estimators': 457,'num_leaves': 49,\n",
    "      }\n",
    "lgbm_reg = LGBMRegressor(**lgbm_params)\n",
    "lgbm_reg.fit(X_train, y_train, verbose=True)\n",
    "\n",
    "y_pred_lgbm = lgbm_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fold Light GMB RMSLE:  0.48670853024263994\n"
     ]
    }
   ],
   "source": [
    "lgbm_RMSLE = rmsle(y_test, y_pred_lgbm)\n",
    "print(' Fold Light GMB RMSLE: ', lgbm_RMSLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.7 [CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.7 \n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.7 \n",
      "\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=5, learning_rate=0.07, colsample_bytree=0.7 \n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=5, learning_rate=0.07, colsample_bytree=0.7 \n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=5, learning_rate=0.07, colsample_bytree=0.7 \n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.7 \n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.7 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=5, learning_rate=0.07, colsample_bytree=0.7, score=(train=0.301, test=0.301), total=10.0min\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 10.9min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.7 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=5, learning_rate=0.07, colsample_bytree=0.7, score=(train=0.302, test=0.297), total=10.3min\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed: 11.0min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.07, colsample_bytree=0.7 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=5, learning_rate=0.07, colsample_bytree=0.7, score=(train=0.301, test=0.300), total=10.6min\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed: 11.0min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.07, colsample_bytree=0.7 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.7, score=(train=0.352, test=0.344), total=14.1min\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed: 14.9min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.07, colsample_bytree=0.7 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.7, score=(train=0.350, test=0.346), total=14.5min\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed: 15.0min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.03, colsample_bytree=0.3 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.7, score=(train=0.352, test=0.350), total=12.8min\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed: 24.4min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.03, colsample_bytree=0.3 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.03, colsample_bytree=0.3, score=(train=0.260, test=0.259), total= 9.5min\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed: 25.5min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.03, colsample_bytree=0.3 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.7, score=(train=0.416, test=0.405), total=25.5min\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed: 26.6min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=5, learning_rate=0.03, colsample_bytree=0.7 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.7, score=(train=0.415, test=0.409), total=25.7min\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed: 26.6min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=5, learning_rate=0.03, colsample_bytree=0.7 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.7, score=(train=0.416, test=0.410), total=26.0min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed: 26.7min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=5, learning_rate=0.03, colsample_bytree=0.7 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.07, colsample_bytree=0.7, score=(train=0.367, test=0.365), total=17.2min\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed: 29.1min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.05, colsample_bytree=0.7 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.07, colsample_bytree=0.7, score=(train=0.367, test=0.360), total=17.7min\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed: 29.3min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.05, colsample_bytree=0.7 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.07, colsample_bytree=0.7, score=(train=0.365, test=0.364), total=17.2min\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed: 32.6min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.05, colsample_bytree=0.7 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=5, learning_rate=0.03, colsample_bytree=0.7, score=(train=-0.293, test=-0.292), total= 9.6min\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed: 37.7min\n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=5, learning_rate=0.03, colsample_bytree=0.7, score=(train=-0.292, test=-0.297), total= 9.8min[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.05, colsample_bytree=0.3 \n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed: 37.7min\n",
      "\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.05, colsample_bytree=0.3 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=5, learning_rate=0.03, colsample_bytree=0.7, score=(train=-0.292, test=-0.291), total=10.2min\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  30 | elapsed: 37.8min remaining: 33.1min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.05, colsample_bytree=0.3 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.03, colsample_bytree=0.3, score=(train=0.260, test=0.256), total=12.8min\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  30 | elapsed: 37.8min remaining: 28.9min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.3 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.03, colsample_bytree=0.3, score=(train=0.262, test=0.262), total=11.8min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  30 | elapsed: 38.6min remaining: 25.7min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.3 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.05, colsample_bytree=0.7, score=(train=0.387, test=0.382), total=20.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  19 out of  30 | elapsed: 51.1min remaining: 29.6min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.3 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.3, score=(train=0.417, test=0.411), total=13.7min\n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.05, colsample_bytree=0.3, score=(train=0.384, test=0.381), total=13.1min\n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.05, colsample_bytree=0.3, score=(train=0.385, test=0.381), total=13.6min[Parallel(n_jobs=-1)]: Done  20 out of  30 | elapsed: 52.4min remaining: 26.2min\n",
      "\n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.05, colsample_bytree=0.7, score=(train=0.390, test=0.380), total=21.9min\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  30 | elapsed: 52.4min remaining: 22.5min\n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.05, colsample_bytree=0.3, score=(train=0.388, test=0.378), total=13.6min[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.05, colsample_bytree=0.7 [Parallel(n_jobs=-1)]: Done  22 out of  30 | elapsed: 52.5min remaining: 19.1min\n",
      "\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.05, colsample_bytree=0.7 [Parallel(n_jobs=-1)]: Done  23 out of  30 | elapsed: 52.5min remaining: 16.0min\n",
      "\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.05, colsample_bytree=0.7 \n",
      "\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  30 | elapsed: 52.5min remaining: 13.1min\n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.3, score=(train=0.417, test=0.406), total=13.4min\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  30 | elapsed: 52.5min remaining: 10.5min\n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.05, colsample_bytree=0.7, score=(train=0.387, test=0.383), total=20.6min\n",
      "[Parallel(n_jobs=-1)]: Done  26 out of  30 | elapsed: 53.6min remaining:  8.2min\n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.3, score=(train=0.413, test=0.408), total= 8.2min\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  30 | elapsed: 59.8min remaining:  6.6min\n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.05, colsample_bytree=0.7, score=(train=0.338, test=0.337), total= 8.1min\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  30 | elapsed: 61.1min remaining:  4.4min\n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.05, colsample_bytree=0.7, score=(train=0.340, test=0.334), total= 8.3min\n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.05, colsample_bytree=0.7, score=(train=0.338, test=0.337), total= 8.4min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 61.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 61.1min finished\n",
      "[16:13:13] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:15:11] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Best parameters found by random search are: {'subsample': 0.7, 'objective': 'reg:linear', 'n_estimators': 100, 'min_child_weight': 4, 'max_depth': 7, 'learning_rate': 0.07, 'colsample_bytree': 0.3}\n",
      "Best score found by random search is: 0.40832756852980534\n"
     ]
    }
   ],
   "source": [
    "# prepare XGBoost regressor and use randomized search for hyperparameter tuning\n",
    "from sklearn.externals.joblib import parallel_backend\n",
    "xgbr = xgb.XGBRegressor()\n",
    "parameters = {'objective':['reg:linear'],\n",
    "              'learning_rate': [.03, 0.05, .07], #so called `eta` value\n",
    "              'max_depth': [5,7],\n",
    "              'min_child_weight': [4],\n",
    "              'subsample': [0.7],\n",
    "              'colsample_bytree': [0.3,0.7],\n",
    "              'n_estimators': [50,100]}\n",
    "rs_xgb = RandomizedSearchCV(xgbr, param_distributions=parameters, cv=3, n_iter=10,\n",
    "                            verbose=100, return_train_score=True)\n",
    "\n",
    "\n",
    "with parallel_backend('threading'):\n",
    "    rs_xgb.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "\n",
    "print('Best parameters found by random search are:', rs_xgb.best_params_)\n",
    "print('Best score found by random search is:', rs_xgb.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:07:19] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[18:09:19] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.07, max_delta_step=0, max_depth=7,\n",
       "             min_child_weight=4, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "             objective='reg:linear', random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "             scale_pos_weight=1, subsample=0.7, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit and get rmsle \n",
    "xgb_param ={'subsample': 0.7, 'objective': 'reg:linear', 'n_estimators': 100, 'min_child_weight': 4, 'max_depth': 7, 'learning_rate': 0.07, 'colsample_bytree': 0.3}\n",
    "xgb_reg = xgb.XGBRegressor(**rsreg_param)\n",
    "xgb_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5776107097698279\n"
     ]
    }
   ],
   "source": [
    "y_pred_xgb = xgb_reg.predict(X_test)\n",
    "xgb_RMSLE = rmsle(y_test, y_pred_xgb)\n",
    "print(xgb_RMSLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Modeling Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on existing methods, I would like to try several models with/without different word embedding methods with Convolutional Neural Networks to see if I can get a lower RMSLE value compared to results from machine learning models. Below I chose to use CNN because it is faster than LSTM according to existing modeling results from others, but it would be interesting to try LSTM in the future for improvements.\n",
    "<br>\n",
    "<br> In this notebook, I only show a CNN model without word embedding (tokenization only on \"name\" and \"item description\" columns). In Part 4, I will specifically show 4 ways of embeddings with CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Mostly same as what I did for machine learning preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from numpy.random import shuffle\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Activation, Flatten, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers import concatenate, BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1482535, 16), (1482535, 16))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load datasets\n",
    "train = pd.read_csv('df_train.csv')\n",
    "test = pd.read_csv('df_train.csv')\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split target variable and train dataset\n",
    "y_train = train['log_price']\n",
    "train.drop(['train_id','log_price'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing following columns:  {'negative', 'name_len', 'positive', 'neutral', 'desc_len', 'compound'}\n"
     ]
    }
   ],
   "source": [
    "# normalize numerical features using min-max normalization\n",
    "cols = set(train.columns.values)\n",
    "skip_cols = {'item_condition_id', 'brand_name',\n",
    "  'shipping', 'item_description', 'main_cat',\n",
    "  'subcat1', 'subcat2','name'}\n",
    "\n",
    "cols_to_normalize = cols - skip_cols\n",
    "print(\"Normalizing following columns: \", cols_to_normalize)\n",
    "\n",
    "def normalize(df):\n",
    "    result=df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        if (feature_name in cols_to_normalize):\n",
    "            max_value = df[feature_name].max()\n",
    "            min_value = df[feature_name].min()\n",
    "            result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result\n",
    "\n",
    "train = normalize(train)\n",
    "test = normalize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1482535/1482535 [00:02<00:00, 499977.24it/s]\n",
      "100%|██████████| 1482535/1482535 [00:03<00:00, 492239.09it/s]\n"
     ]
    }
   ],
   "source": [
    "def clean_cat(cat_values):\n",
    "    '''takes categorical column values as arguments and returns list of cleaned categories'''\n",
    "    \n",
    "    catogories = list(cat_values)\n",
    "\n",
    "    cat_list = []\n",
    "    for i in tqdm(catogories):\n",
    "        i = re.sub('[^A-Za-z0-9]+', ' ', i)\n",
    "        i = i.replace(' ','')\n",
    "        i = i.replace('&','_')\n",
    "        cat_list.append(i.strip())\n",
    "    \n",
    "    return cat_list\n",
    "\n",
    "# Cleaning brand name before using count vectorizer\n",
    "# Using same preprocessing as used earlier for categories: 'clean_cat()' function\n",
    "\n",
    "train['brand_name'] = clean_cat(train['brand_name'].values)\n",
    "test['brand_name'] = clean_cat(test['brand_name'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding categorical variables for embedding\n",
    "label = LabelEncoder()\n",
    "\n",
    "label.fit(train.brand_name) # brand name \n",
    "train.brand_name = label.transform(train.brand_name)\n",
    "\n",
    "label.fit(train.main_cat) # sub_cat0\n",
    "train.main_cat = label.transform(train.main_cat)\n",
    "\n",
    "label.fit(train.subcat1) # sub_cat_1\n",
    "train.subcat1 = label.transform(train.subcat1)\n",
    "\n",
    "label.fit(train.subcat2) # sub_cat2\n",
    "train.subcat2 = label.transform(train.subcat2)\n",
    "\n",
    "del label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, I chose to encode categorical variables using label encoder because I want to avoid high memory consumption that could lead to slow training on deep learning models. It is better to use a one-hot-encoding if possible, as the categories are not ordinal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1482535/1482535 [00:05<00:00, 285069.64it/s]\n",
      "100%|██████████| 1482535/1482535 [00:05<00:00, 279610.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# define function to preprocess \"name\"\n",
    "def preprocess_name(text_col):\n",
    "    preprocessed_names = []\n",
    "    for sentence in tqdm(text_col.values):\n",
    "        sent = sentence.replace('\\\\r', ' ')\n",
    "        sent = sent.replace('\\\\\"', ' ')\n",
    "        sent = sent.replace('\\\\n', ' ')\n",
    "        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "        preprocessed_names.append(sent.lower().strip())\n",
    "    return preprocessed_names\n",
    "\n",
    "# preprocess \"name\"\n",
    "train['name'] = preprocess_name(train['name'])\n",
    "test['name'] = preprocess_name(test['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text to Sequence Data.\n",
    "# combining columns, item_description, name, category_name.\n",
    "full_text = np.hstack([train.item_description.str.lower(), train.name.str.lower()])\n",
    "# full_text_test = np.hstack([test.item_description.str.lower(), test.name.str.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(full_text)\n",
    "train['seq_desc'] = tokenizer.texts_to_sequences(train.item_description.str.lower()) \n",
    "train['seq_name'] = tokenizer.texts_to_sequences(train.name.str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding out max len of all the text data combined\n",
    "max_len_brand = np.max(train.brand_name.max())    # brand \n",
    "max_len_condition = np.max(int(max(train.item_condition_id)))  # item_condition\n",
    "max_len_desc = np.max(int(train.desc_len.max()))  # desc_len\n",
    "max_len_name = np.max(int(train.name_len.max()))  # name_len\n",
    "max_len_sub0 = np.max(int(train.main_cat.max()))  # main_cat\n",
    "max_len_sub1 = np.max(int(train.subcat1.max()))  # Sub_1\n",
    "max_len_sub2 = np.max(train.subcat2.max())  # Sub_2\n",
    "# defining max length for padding text data\n",
    "name_padding = 15\n",
    "description_padding = 80\n",
    "max_len = np.max([np.max(train.seq_name.max()),np.max(train.seq_desc.max()),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test split\n",
    "x_tr, x_te, Y_train, Y_test = train_test_split(train,y_train, random_state=42, train_size=0.7)\n",
    "Y_train = Y_train.values.reshape(-1, 1)\n",
    "Y_test = Y_test.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN without word embeddings as weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "x_train_padded = { \n",
    "\"name\" : pad_sequences(x_tr.seq_name, maxlen= name_padding),\n",
    "\"item_desc\" : pad_sequences(x_tr.seq_desc, maxlen= description_padding),\n",
    "\"brand_name\" : np.array(x_tr.brand_name),\n",
    "\"item_condition\" : np.array(x_tr.item_condition_id),\n",
    "\"shipping\" : np.array(x_tr[[\"shipping\"]]),\n",
    "\"desc_len\" : np.array(x_tr[[\"desc_len\"]]),\n",
    "\"name_len\" : np.array(x_tr[[\"name_len\"]]),\n",
    "\"subcat_0\" : np.array(x_tr.main_cat),\n",
    "\"subcat_1\"  : np.array(x_tr.subcat1),\n",
    "\"subcat_2\" : np.array(x_tr.subcat2),\n",
    "} \n",
    "x_test_padded = {\n",
    "\"name\" : pad_sequences(x_te.seq_name, maxlen= name_padding),\n",
    "\"item_desc\" : pad_sequences(x_te.seq_desc, maxlen= description_padding),\n",
    "\"brand_name\" : np.array(x_te.brand_name),\n",
    "\"item_condition\" : np.array(x_te.item_condition_id),\n",
    "\"shipping\" : np.array(x_te[[\"shipping\"]]),\n",
    "\"desc_len\" : np.array(x_te[[\"desc_len\"]]),\n",
    "\"name_len\" : np.array(x_te[[\"name_len\"]]),\n",
    "\"subcat_0\" : np.array(x_te.main_cat),\n",
    "\"subcat_1\" : np.array(x_te.subcat1),\n",
    "\"subcat_2\" : np.array(x_te.subcat2),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = x_train_padded\n",
    "x_te = x_test_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define attributes\n",
    "batch_size = 512 * 3\n",
    "epochs = 2\n",
    "exp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n",
    "steps = int(len(x_tr['name']) / batch_size) * epochs\n",
    "lr_init, lr_fin = 0.005, 0.001\n",
    "lr_decay = exp_decay(lr_init, lr_fin, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definign inputs\n",
    "subcat_0 = Input(shape=[1], name=\"subcat_0\")\n",
    "subcat_1 = Input(shape=[1], name=\"subcat_1\")\n",
    "subcat_2 = Input(shape=[1], name=\"subcat_2\")\n",
    "desc_len = Input(shape=[1], name=\"desc_len\")\n",
    "name_len = Input(shape=[1], name=\"name_len\")\n",
    "\n",
    "brand_name = Input(shape=[1], name=\"brand_name\")\n",
    "\n",
    "num_vars = Input(shape=[1], name=\"shipping\")\n",
    "\n",
    "item_condition = Input(shape=[1], name=\"item_condition\")\n",
    "\n",
    "name = Input(shape=[x_tr[\"name\"].shape[1]], name=\"name\") # 15 shape = [15]\n",
    "item_desc = Input(shape=[x_tr[\"item_desc\"].shape[1]], name=\"item_desc\") # 80 shape = [80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define embedding layers, max lengthes raised larger becuase of the system error using tensorflow 2.x\n",
    "sub0_emb = Embedding(max_len_sub0+10, 10)(subcat_0)\n",
    "sub1_emb = Embedding(max_len_sub1+10, 10)(subcat_1)\n",
    "sub2_emb = Embedding(max_len_sub2+10, 10)(subcat_2)\n",
    "\n",
    "brand_emb = Embedding(max_len_brand+10, 10)(brand_name)\n",
    "\n",
    "item_cond_emb = Embedding(max_len_condition+10, 5)(item_condition)\n",
    "\n",
    "name_emb = Embedding(max_len+10, 15, weights=[embedding_matrix])(name)\n",
    "item_desc_emb = Embedding(max_len+10, 80, weights=[embedding_matrix])(item_desc)\n",
    "\n",
    "desc_len_emb = Embedding(max_len_desc+10, 5)(desc_len)\n",
    "name_len_emb = Embedding(max_len_name+10, 5)(name_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn and pooling layers\n",
    "convs1 = []\n",
    "convs2 = []\n",
    "for filter_length in [1,2]:\n",
    "  cnn_layer1 = Conv1D(filters=50, kernel_size=filter_length, padding='same', activation='relu', strides=1) (name_emb)\n",
    "  cnn_layer2 = Conv1D(filters=50, kernel_size=filter_length, padding='same', activation='relu', strides=1) (item_desc_emb)\n",
    "  maxpool1 = GlobalMaxPooling1D() (cnn_layer1)\n",
    "  maxpool2 = GlobalMaxPooling1D() (cnn_layer2)\n",
    "  convs1.append(maxpool1)\n",
    "  convs2.append(maxpool2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs1 = concatenate(convs1)\n",
    "convs2 = concatenate(convs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten\n",
    "flat_1 = Flatten() (brand_emb)\n",
    "flat_2 = Flatten() (item_cond_emb)\n",
    "flat_5 = Flatten() (sub0_emb)\n",
    "flat_6 = Flatten() (sub1_emb)\n",
    "flat_7 = Flatten() (sub2_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate for further steps\n",
    "main_l = concatenate([flat_1,flat_2,flat_5,flat_6,flat_7,embed_1,convs1,convs2, num_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dropout and batchnormalization layers\n",
    "main_l = Dropout(0.1)(Dense(256,kernel_initializer='normal',activation='relu') (main_l))\n",
    "main_l = BatchNormalization()(Dense(128,kernel_initializer='normal',activation='relu') (main_l))\n",
    "main_l = Dropout(0.1)(Dense(64,kernel_initializer='normal',activation='relu') (main_l))\n",
    "main_l = BatchNormalization()(Dense(32,kernel_initializer='normal',activation='relu') (main_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_23\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "name (InputLayer)               [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_desc (InputLayer)          [(None, 80)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_104 (Embedding)       (None, 15, 15)       3749190     name[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_105 (Embedding)       (None, 80, 80)       19995680    item_desc[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "brand_name (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_condition (InputLayer)     [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "subcat_0 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "subcat_1 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "subcat_2 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 15, 50)       800         embedding_104[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 15, 50)       1550        embedding_104[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 80, 50)       4050        embedding_105[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 80, 50)       8050        embedding_105[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_102 (Embedding)       (None, 1, 10)        48100       brand_name[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_103 (Embedding)       (None, 1, 5)         75          item_condition[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_99 (Embedding)        (None, 1, 10)        190         subcat_0[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_100 (Embedding)       (None, 1, 10)        1220        subcat_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_101 (Embedding)       (None, 1, 10)        8790        subcat_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_44 (Global (None, 50)           0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_46 (Global (None, 50)           0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_45 (Global (None, 50)           0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_47 (Global (None, 50)           0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_55 (Flatten)            (None, 10)           0           embedding_102[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_56 (Flatten)            (None, 5)            0           embedding_103[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_57 (Flatten)            (None, 10)           0           embedding_99[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_58 (Flatten)            (None, 10)           0           embedding_100[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_59 (Flatten)            (None, 10)           0           embedding_101[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 100)          0           global_max_pooling1d_44[0][0]    \n",
      "                                                                 global_max_pooling1d_46[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 100)          0           global_max_pooling1d_45[0][0]    \n",
      "                                                                 global_max_pooling1d_47[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "shipping (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 246)          0           flatten_55[0][0]                 \n",
      "                                                                 flatten_56[0][0]                 \n",
      "                                                                 flatten_57[0][0]                 \n",
      "                                                                 flatten_58[0][0]                 \n",
      "                                                                 flatten_59[0][0]                 \n",
      "                                                                 concatenate_33[0][0]             \n",
      "                                                                 concatenate_34[0][0]             \n",
      "                                                                 shipping[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_56 (Dense)                (None, 256)          63232       concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 256)          0           dense_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_57 (Dense)                (None, 128)          32896       dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 128)          512         dense_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_58 (Dense)                (None, 64)           8256        batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 64)           0           dense_58[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_59 (Dense)                (None, 32)           2080        dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 32)           128         dense_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "desc_len (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "name_len (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_60 (Dense)                (None, 1)            33          batch_normalization_23[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 23,924,832\n",
      "Trainable params: 23,924,512\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define output layer and optimizer\n",
    "output = Dense(1, activation=\"linear\") (main_l)\n",
    "model_2 = Model([name, item_desc, brand_name , desc_len,name_len, item_condition, subcat_0, subcat_1, subcat_2, num_vars ], output)\n",
    "optimizer = optimizers.Adam(lr = 0.005)\n",
    "# compile the model\n",
    "model_2.compile(loss = 'mse', optimizer = optimizer)\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "676/676 [==============================] - 260s 385ms/step - loss: 0.5104\n",
      "Epoch 2/2\n",
      "676/676 [==============================] - 248s 367ms/step - loss: 0.1813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fee9ad5e290>"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit cnn model\n",
    "model_2.fit(x_tr, Y_train, epochs= 2, batch_size= 512 * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RMSLE error: 0.45484033271260815\n"
     ]
    }
   ],
   "source": [
    "# get the error\n",
    "y_pred = model_2.predict(x_te, batch_size=batch_size)\n",
    "print(\"RMSLE error:\", rmsle(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison and Take Aways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "scores['ridge'] = 0.47\n",
    "scores['lightGBM'] = 0.48\n",
    "scores['randome forest'] = 0.7\n",
    "scores['cnn'] = 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x864 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot RMSEs\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style(\"white\")\n",
    "fig = plt.figure(figsize=(24, 12))\n",
    "\n",
    "ax = sns.pointplot(x=list(scores.keys()), y= list(scores.values()), markers=['o'], linestyles=['-'])\n",
    "# for i, score in enumerate(scores.values()):\n",
    "#     ax.text(i, score + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n",
    "\n",
    "plt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\n",
    "plt.xlabel('Model', size=20, labelpad=12.5)\n",
    "plt.tick_params(axis='x', labelsize=13.5)\n",
    "plt.tick_params(axis='y', labelsize=12.5)\n",
    "\n",
    "plt.title('Scores of Models', size=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is obvious that in machine learning methods, ridge regression did the best. And generally speaking, deep learning in cnn model performed the best among all models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for future works, multi-layer percepton model is very attrctive to try since it does not envolve a lot of calculations and it is time-saving as well. On the other hand, to get more information from the modeling results, it is attempting to map column names to sparse matrix and then get the most important coefficients from the fitted ridge regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
