{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mercari Price Prediction Project \n",
    "##  - Part 3 Data Processing, Machine Learning Modeling, and CNN without Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> **Error Metric:** RMSLE (Root Mean Square Logarithmic Error)\n",
    "<br>**Real world/Business Objectives and Constraints:**\n",
    "<br> 1. Predict the price of an item given its condition, description and other related features.\n",
    "<br> 2. Minimize the difference between predicted and actual price (RMSLE)\n",
    "<br> 3. Try to provide some interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inaba3910/opt/anaconda3/lib/python3.7/site-packages/dask/dataframe/utils.py:14: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import scipy\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# modeling\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge,Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# word embedding\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# tuning and processing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# support\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1482535, 16), (693359, 15))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('df_train.csv')\n",
    "test = pd.read_csv('df_test.csv')\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing \n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "# we are removing the words from the stop words list: 'no', 'nor', 'not'\n",
    "stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no description yet'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing 'item_description' for train set\n",
    "preprocessed_desc_train = []\n",
    "for sentance in train['item_description'].values:\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    sent = re.sub('[0-9]',' ',sent)\n",
    "    sent = re.sub('[^0-9A-Za-z]+',' ', sent)\n",
    "    sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n",
    "    preprocessed_desc_train.append(sent.lower().strip())\n",
    "\n",
    "# after preprocesing\n",
    "preprocessed_desc_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['item_description'] = preprocessed_desc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ava viv blouse'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing 'name' for train set\n",
    "preprocessed_name_train = []\n",
    "for sentance in train['name'].values:\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    sent = re.sub('[0-9]',' ',sent)\n",
    "    sent = re.sub('[^0-9A-Za-z]+',' ', sent)\n",
    "    sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n",
    "    preprocessed_name_train.append(sent.lower().strip())\n",
    "\n",
    "# after preprocesing\n",
    "preprocessed_name_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['name'] = preprocessed_name_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_id             False\n",
       "name                 False\n",
       "item_condition_id    False\n",
       "log_price            False\n",
       "brand_name           False\n",
       "shipping             False\n",
       "item_description     False\n",
       "name_len             False\n",
       "desc_len             False\n",
       "main_cat             False\n",
       "subcat1              False\n",
       "subcat2              False\n",
       "negative             False\n",
       "neutral              False\n",
       "positive             False\n",
       "compound             False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check before split\n",
    "train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing 'item_description' for test set\n",
    "preprocessed_desc_test = []\n",
    "for sentance in test['item_description'].values:\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    sent = re.sub('[0-9]',' ',sent)\n",
    "    sent = re.sub('[^0-9A-Za-z]+',' ', sent)\n",
    "    sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n",
    "    preprocessed_desc_test.append(sent.lower().strip())\n",
    "# after preprocesing\n",
    "preprocessed_desc_test[20000]\n",
    "test['item_description'] = preprocessed_desc_test\n",
    "\n",
    "# preprocessing 'name' for test set\n",
    "preprocessed_name_test = []\n",
    "for sentance in test['name'].values:\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    sent = re.sub('[0-9]',' ',sent)\n",
    "    sent = re.sub('[^0-9A-Za-z]+',' ', sent)\n",
    "    sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n",
    "    preprocessed_name_test.append(sent.lower().strip())\n",
    "\n",
    "# after preprocesing\n",
    "preprocessed_name_test[20000]\n",
    "\n",
    "test['name'] = preprocessed_name_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_id              False\n",
       "name                 False\n",
       "item_condition_id    False\n",
       "brand_name           False\n",
       "shipping             False\n",
       "item_description     False\n",
       "name_len             False\n",
       "desc_len             False\n",
       "main_cat             False\n",
       "subcat1              False\n",
       "subcat2              False\n",
       "negative             False\n",
       "neutral              False\n",
       "positive             False\n",
       "compound             False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into train and test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate target variable and features for training\n",
    "y_train = train['log_price']\n",
    "train.drop(['train_id','log_price'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train \n",
    "y=y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing on train and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot-Encoding on Categorical Features using CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed from previous analysis, there are numbers of categorical variables in this dataset, and before moving into machine learning, we need to convert them into numerical format. Here, I chose to use One-Hot Encoding specifically done with respect to train data to avoid data leakage issue. I did not include test data into it because test data is unseen to us. So, if any category appears new while testing, we will ignore that value while converting to One-Hot encoded form.\n",
    "<br>\n",
    "<br>So, here we have ‘Brand Name’, ‘General Category’, ‘Sub category 1’, ‘Sub category 2’ as columns with categorical values. We will convert these categorical values into one-hot encoded form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning function\n",
    "def clean_cat(cat_values):\n",
    "    '''takes categorical column values as arguments and returns list of cleaned categories'''\n",
    "    \n",
    "    catogories = list(cat_values)\n",
    "\n",
    "    cat_list = []\n",
    "    for i in tqdm(catogories):\n",
    "        i = re.sub('[^A-Za-z0-9]+', ' ', i)\n",
    "        i = i.replace(' ','')\n",
    "        i = i.replace('&','_')\n",
    "        cat_list.append(i.strip())\n",
    "    \n",
    "    return cat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1037774/1037774 [00:02<00:00, 518193.22it/s]\n",
      "100%|██████████| 444761/444761 [00:00<00:00, 519002.27it/s]\n",
      "100%|██████████| 693359/693359 [00:01<00:00, 540885.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# cleaning brand name before using count vectorizer\n",
    "X_train['brand_name'] = clean_cat(X_train['brand_name'].values)\n",
    "X_test['brand_name'] = clean_cat(X_test['brand_name'].values)\n",
    "test['brand_name'] = clean_cat(test['brand_name'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrices after one hot encoding\n",
      "(1037774, 4388) \n",
      " (444761, 4388) \n",
      " (693359, 4388)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding brand name\n",
    "vectorizer = CountVectorizer(lowercase=False, binary=True)\n",
    "xtrain_brand_onehot = vectorizer.fit_transform(X_train['brand_name'])\n",
    "xtest_brand_onehot = vectorizer.transform(X_test['brand_name'])\n",
    "test_brand_onehot = vectorizer.transform(test['brand_name'])\n",
    "\n",
    "print(\"Shape of matrices after one hot encoding\")\n",
    "print(xtrain_brand_onehot.shape, \"\\n\", xtest_brand_onehot.shape, \"\\n\", test_brand_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrices after one hot encoding\n",
      "(1037774, 12) \n",
      " (444761, 12) \n",
      " (693359, 12)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding main_cat\n",
    "vectorizer = CountVectorizer(lowercase=False, binary=True)\n",
    "xtrain_maincat_onehot = vectorizer.fit_transform(X_train['main_cat'].values)\n",
    "xtest_maincat_onehot = vectorizer.transform(X_test['main_cat'].values)\n",
    "test_maincat_onehot = vectorizer.transform(test['main_cat'].values)\n",
    "\n",
    "print(\"Shape of matrices after one hot encoding\")\n",
    "print(xtrain_maincat_onehot.shape, \"\\n\", xtest_maincat_onehot.shape, \"\\n\", test_maincat_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrices after one hot encoding\n",
      "(1037774, 142) \n",
      " (444761, 142) \n",
      " (693359, 142)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding subcat1\n",
    "vectorizer = CountVectorizer(lowercase=False, binary=True)\n",
    "xtrain_subcat1_onehot = vectorizer.fit_transform(X_train['subcat1'].values)\n",
    "xtest_subcat1_onehot = vectorizer.transform(X_test['subcat1'].values)\n",
    "test_subcat1_onehot = vectorizer.transform(test['subcat1'].values)\n",
    "print(\"Shape of matrices after one hot encoding\")\n",
    "print(xtrain_subcat1_onehot.shape, \"\\n\", xtest_subcat1_onehot.shape, \"\\n\", test_subcat1_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrices after one hot encoding\n",
      "(1037774, 951) \n",
      " (444761, 951) \n",
      " (693359, 951)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding subcat2\n",
    "vectorizer = CountVectorizer(lowercase=False, binary=True)\n",
    "xtrain_subcat2_onehot = vectorizer.fit_transform(X_train['subcat2'].values)\n",
    "xtest_subcat2_onehot = vectorizer.transform(X_test['subcat2'].values)\n",
    "test_subcat2_onehot = vectorizer.transform(test['subcat2'].values)\n",
    "\n",
    "print(\"Shape of matrices after one hot encoding\")\n",
    "print(xtrain_subcat2_onehot.shape, \"\\n\", xtest_subcat2_onehot.shape, \"\\n\", test_subcat2_onehot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf Vectorization on Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns 'name' and 'item_description' were encoded into TF-IDF vectors of uni-grams, bi-grams and tri-grams. Note that using 1,2,3-grams together would result in a huge number of words in the dictionary of TF-IDF vectorizer and using all of them would result in very high dimensional vectors. To avoid this, I have limited the number of dimensions to 250k for name and 500k for item_description vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrices after vectorization\n",
      "(1037774, 250000) \n",
      " (444761, 250000) \n",
      " (693359, 250000)\n"
     ]
    }
   ],
   "source": [
    "# tfidf encoding 'name'\n",
    "tfidfvectorizer = TfidfVectorizer(ngram_range=(1, 3), min_df=3, max_features=250000)\n",
    "\n",
    "xtrain_name_tfidf = tfidfvectorizer.fit_transform(X_train['name'].values)\n",
    "xtest_name_tfidf = tfidfvectorizer.transform(X_test['name'].values)\n",
    "test_name_tfidf = tfidfvectorizer.transform(test['name'].values)\n",
    "\n",
    "print(\"Shape of matrices after vectorization\")\n",
    "print(xtrain_name_tfidf.shape, \"\\n\", xtest_name_tfidf.shape, \"\\n\", test_name_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrices after vectorization\n",
      "(1037774, 500000) \n",
      " (444761, 500000) \n",
      " (693359, 500000)\n"
     ]
    }
   ],
   "source": [
    "# tfidf encoding 'item_description'\n",
    "tfidfvectorizer = TfidfVectorizer(ngram_range=(1, 3), min_df=5, max_features=500000)\n",
    "xtrain_desc_tfidf = tfidfvectorizer.fit_transform(X_train['item_description'].values)\n",
    "xtest_desc_tfidf = tfidfvectorizer.transform(X_test['item_description'].values)\n",
    "test_desc_tfidf = tfidfvectorizer.transform(test['item_description'].values)\n",
    "\n",
    "print(\"Shape of matrices after vectorization\")\n",
    "print(xtrain_desc_tfidf.shape, \"\\n\", xtest_desc_tfidf.shape, \"\\n\", test_desc_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary purpose of normalization is to scale numeric data from different columns down to an equivalent scale so that the model doesn’t get skewed due to huge variance in a few columns. I have used min-max normalization here because we need some power to supress the effect of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing following columns:  {'desc_len', 'compound', 'positive', 'negative', 'neutral', 'name_len'}\n"
     ]
    }
   ],
   "source": [
    "# pick up numerical columns\n",
    "cols = set(X_train.columns.values)\n",
    "skip_cols = {'item_condition_id', 'brand_name',\n",
    "  'shipping', 'item_description', 'main_cat',\n",
    "  'subcat1', 'subcat2','name'}\n",
    "\n",
    "cols_to_normalize = cols - skip_cols\n",
    "print(\"Normalizing following columns: \", cols_to_normalize)\n",
    "\n",
    "# define min-max normalization funtion\n",
    "def normalize(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        if (feature_name in cols_to_normalize):\n",
    "            max_value = df[feature_name].max()\n",
    "            min_value = df[feature_name].min()\n",
    "            result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize numerical features\n",
    "xtrain_normalized = normalize(X_train)\n",
    "xtest_normalized = normalize(X_test)\n",
    "test_normalized = normalize(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidate All Features to a Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating and storing all numerical features\n",
    "X_xtrain = xtrain_normalized[list(cols_to_normalize)]\n",
    "X_xtest = xtest_normalized[list(cols_to_normalize)]\n",
    "X_test_x = test_normalized[list(cols_to_normalize)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1037774, 2) (444761, 2) (693359, 2)\n"
     ]
    }
   ],
   "source": [
    "# storing categorical features to sparse matrix\n",
    "X_xtrain_cat = csr_matrix(pd.get_dummies(xtrain_normalized[['item_condition_id', 'shipping']], sparse=True).values)\n",
    "X_xtest_cat = csr_matrix(pd.get_dummies(xtest_normalized[['item_condition_id', 'shipping']], sparse=True).values)\n",
    "X_test_cat = csr_matrix(pd.get_dummies(test_normalized[['item_condition_id', 'shipping']], sparse=True).values)\n",
    "print(X_xtrain_cat.shape, X_xtest_cat.shape, X_test_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1037774, 755495) (444761, 755495) (693359, 755495)\n"
     ]
    }
   ],
   "source": [
    "# stack all categorical and text sparse matrices\n",
    "xtrain_sparse = hstack((xtrain_brand_onehot, xtrain_maincat_onehot, xtrain_subcat1_onehot, xtrain_subcat2_onehot, \\\n",
    "              xtrain_name_tfidf, xtrain_desc_tfidf, X_xtrain_cat)).tocsr()\n",
    "xtest_sparse = hstack((xtest_brand_onehot, xtest_maincat_onehot, xtest_subcat1_onehot, xtest_subcat2_onehot, \\\n",
    "             xtest_name_tfidf, xtest_desc_tfidf, X_xtest_cat)).tocsr()\n",
    "test_sparse = hstack((test_brand_onehot, test_maincat_onehot, test_subcat1_onehot, test_subcat2_onehot, \\\n",
    "                      test_name_tfidf, test_desc_tfidf, X_test_cat)).tocsr()\n",
    "print(xtrain_sparse.shape, xtest_sparse.shape, test_sparse.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (1037774, 755501), CV size: (444761, 755501), Test size: (693359, 755501)\n"
     ]
    }
   ],
   "source": [
    "# stack dense feature matrix with categorical and text vectors\n",
    "X_train = hstack((X_xtrain.values, xtrain_sparse)).tocsr()\n",
    "X_test = hstack((X_xtest.values, xtest_sparse)).tocsr()\n",
    "test_x = hstack((X_test_x.values, test_sparse)).tocsr()\n",
    "print('Train size: {}, CV size: {}, Test size: {}' .format(X_train.shape, X_test.shape, test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete variables we do not need further\n",
    "del vectorizer\n",
    "del xtrain_normalized, xtest_normalized, test_normalized\n",
    "del xtrain_brand_onehot, xtrain_maincat_onehot, xtrain_subcat1_onehot, xtrain_subcat2_onehot, X_xtrain_cat,xtrain_name_tfidf, xtrain_desc_tfidf\n",
    "\n",
    "del xtest_brand_onehot, xtest_maincat_onehot, xtest_subcat1_onehot, xtest_subcat2_onehot, X_xtest_cat, xtest_name_tfidf, xtest_desc_tfidf\n",
    "\n",
    "del test_brand_onehot, test_maincat_onehot, test_subcat1_onehot, test_subcat2_onehot, X_test_cat, test_name_tfidf, test_desc_tfidf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Error Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y, y_pred): # return Rmsle value.\n",
    "    return np.sqrt(np.mean(np.square(y_pred - y )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Modeling Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I chose to use ridge regression, lasso regression, light gradient boosting regressor, and xgboost regressor with randomized search for hyperparameter tuning as 4 of the machine learning models. These were chosen particularly because we need fast and robust regression model towards price, and we need a model with a high performance so some models like random forest that is known better performed in categorization problems are excluded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=0.0001 ..................[CV] solver=lsqr, fit_intercept=False, alpha=0.0001 ..................\n",
      "\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=0.0001 ..................\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=0.001 ...................\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=0.001 ...................\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=0.001 ...................\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=0.01 ....................[CV] solver=lsqr, fit_intercept=False, alpha=0.01 ....................\n",
      "\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=0.001, score=(train=-0.198, test=-0.223), total=23.3min[CV]  solver=lsqr, fit_intercept=False, alpha=0.0001, score=(train=-0.198, test=-0.223), total=23.3min\n",
      "\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=0.01 ....................[CV] solver=lsqr, fit_intercept=False, alpha=0.1 .....................\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 23.6min\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed: 23.6min\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=0.01, score=(train=-0.195, test=-0.225), total=23.8min[CV]  solver=lsqr, fit_intercept=False, alpha=0.001, score=(train=-0.195, test=-0.223), total=23.8min[CV]  solver=lsqr, fit_intercept=False, alpha=0.01, score=(train=-0.195, test=-0.223), total=23.8min\n",
      "\n",
      "\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=0.1 .....................\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=0.1 .....................[CV] solver=lsqr, fit_intercept=False, alpha=1 .......................\n",
      "\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=0.0001, score=(train=-0.195, test=-0.225), total=23.8min\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed: 24.2min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=1 .......................[CV]  solver=lsqr, fit_intercept=False, alpha=0.0001, score=(train=-0.195, test=-0.223), total=23.8min\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed: 24.2min\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed: 24.2min\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed: 24.2min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=1 .......................[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed: 24.2min\n",
      "\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=0.001, score=(train=-0.195, test=-0.225), total=23.9min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=10 ......................[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed: 24.3min\n",
      "\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=1, score=(train=-0.198, test=-0.223), total= 6.9min\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed: 31.8min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=10 ......................\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=10, score=(train=-0.212, test=-0.232), total= 5.1min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed: 37.0min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=10 ......................\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=10, score=(train=-0.213, test=-0.228), total= 5.2min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=100 .....................\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed: 42.4min\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=10, score=(train=-0.214, test=-0.231), total=18.4min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=100 .....................[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed: 44.9min\n",
      "\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=100, score=(train=-0.258, test=-0.263), total= 3.0min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=100 .....................[Parallel(n_jobs=-1)]: Done  13 out of  27 | elapsed: 45.9min remaining: 49.4min\n",
      "\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=0.01, score=(train=-0.198, test=-0.223), total=24.2min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=1000 ....................[Parallel(n_jobs=-1)]: Done  14 out of  27 | elapsed: 49.9min remaining: 46.3min\n",
      "\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=0.1, score=(train=-0.195, test=-0.223), total=24.9min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  27 | elapsed: 50.0min remaining: 40.0min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=1000 ....................\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=0.1, score=(train=-0.198, test=-0.223), total=24.0min\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  27 | elapsed: 50.3min remaining: 34.6min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=1000 ....................\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=0.1, score=(train=-0.195, test=-0.225), total=24.2min\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  27 | elapsed: 50.4min remaining: 29.6min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=10000 ...................\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=1, score=(train=-0.195, test=-0.225), total=24.2min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  27 | elapsed: 50.4min remaining: 25.2min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=10000 ...................\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=1, score=(train=-0.195, test=-0.222), total=25.0min\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  27 | elapsed: 52.0min remaining: 21.9min\n",
      "[CV] solver=lsqr, fit_intercept=False, alpha=10000 ...................\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=1000, score=(train=-0.325, test=-0.330), total= 2.2min\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  27 | elapsed: 52.8min remaining: 18.5min\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=10000, score=(train=-0.448, test=-0.446), total= 1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  27 | elapsed: 53.6min remaining: 15.3min\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=10000, score=(train=-0.447, test=-0.451), total= 4.1min\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  27 | elapsed: 55.2min remaining: 12.5min\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=10000, score=(train=-0.448, test=-0.447), total= 3.5min\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  27 | elapsed: 55.3min remaining:  9.6min\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=1000, score=(train=-0.327, test=-0.326), total= 5.2min\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  27 | elapsed: 55.9min remaining:  7.0min\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=1000, score=(train=-0.326, test=-0.326), total= 5.7min\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  27 | elapsed: 56.1min remaining:  4.5min\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=100, score=(train=-0.256, test=-0.265), total=10.9min\n",
      "[CV]  solver=lsqr, fit_intercept=False, alpha=100, score=(train=-0.259, test=-0.262), total= 9.9min\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed: 56.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed: 56.2min finished\n"
     ]
    }
   ],
   "source": [
    "# prepare lasso regression and use randomized search for hyperparameter tuning\n",
    "# here, use parallel_backend to boost speed and avoid no end tuning process\n",
    "from sklearn.externals.joblib import parallel_backend\n",
    "parameters = {'alpha':[0.0001,0.001,0.01,0.1,1,10,100,1000,10000],\n",
    "              'fit_intercept' : [False],\n",
    "              'solver' : ['lsqr']}\n",
    "model = Ridge()\n",
    "rs_ridge = RandomizedSearchCV(model,\n",
    "                        param_distributions = parameters,\n",
    "                        cv = 3, \n",
    "                        scoring = 'neg_mean_squared_error',\n",
    "                        verbose = 100,\n",
    "                        return_train_score = True,\n",
    "                        n_iter=10, \n",
    "                        random_state=42)\n",
    "\n",
    "with parallel_backend('threading'):\n",
    "    rs_ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by random search are: {'solver': 'lsqr', 'fit_intercept': False, 'alpha': 1}\n",
      "Best score found by random search is: -0.2232724389821035\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters found by random search are:', rs_ridge.best_params_)\n",
    "print('Best score found by random search is:', rs_ridge.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and get rmsle\n",
    "ridgeReg = Ridge(alpha=1,solver = \"lsqr\", fit_intercept=False )\n",
    "ridgeReg.fit(X_train, y_train)\n",
    "y_pred_ridge = ridgeReg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fold %02d Ridge RMSLE:  0.48015452226677147\n"
     ]
    }
   ],
   "source": [
    "ridge_RMSLE = rmsle(y_test, y_pred_ridge)\n",
    "print(' Fold %02d Ridge RMSLE: ', ridge_RMSLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[CV] fit_intercept=False, alpha=0.0001 ...............................[CV] fit_intercept=False, alpha=0.0001 ...............................\n",
      "[CV] fit_intercept=False, alpha=0.0001 ...............................\n",
      "\n",
      "[CV] fit_intercept=False, alpha=0.001 ................................[CV] fit_intercept=False, alpha=0.001 ................................\n",
      "[CV] fit_intercept=False, alpha=0.001 ................................\n",
      "[CV] fit_intercept=False, alpha=0.01 .................................\n",
      "[CV] fit_intercept=False, alpha=0.01 .................................\n",
      "\n",
      "[CV]  fit_intercept=False, alpha=0.01, score=(train=-0.505, test=-0.505), total= 2.9min\n",
      "[CV]  fit_intercept=False, alpha=0.01, score=(train=-0.505, test=-0.508), total= 2.9min\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  3.1min\n",
      "[CV] fit_intercept=False, alpha=0.1 ..................................[CV] fit_intercept=False, alpha=0.01 .................................\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  3.1min\n",
      "[CV]  fit_intercept=False, alpha=0.1, score=(train=-0.791, test=-0.788), total=  44.5s\n",
      "[CV] fit_intercept=False, alpha=0.1 ..................................\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:  3.9min\n",
      "[CV]  fit_intercept=False, alpha=0.001, score=(train=-0.392, test=-0.389), total= 4.1min\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:  4.3min\n",
      "[CV] fit_intercept=False, alpha=0.1 ..................................\n",
      "[CV]  fit_intercept=False, alpha=0.001, score=(train=-0.389, test=-0.393), total= 4.1min\n",
      "[CV] fit_intercept=False, alpha=1 ....................................\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  4.3min\n",
      "[CV]  fit_intercept=False, alpha=1, score=(train=-2.416, test=-2.409), total=  25.9s\n",
      "[CV] fit_intercept=False, alpha=1 ....................................[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:  4.8min\n",
      "\n",
      "[CV]  fit_intercept=False, alpha=0.1, score=(train=-0.788, test=-0.794), total=  52.1s\n",
      "[CV] fit_intercept=False, alpha=1 ....................................\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:  4.9min\n",
      "[CV]  fit_intercept=False, alpha=0.1, score=(train=-0.792, test=-0.789), total=  38.0s\n",
      "[CV] fit_intercept=False, alpha=10 ...................................[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:  5.0min\n",
      "\n",
      "[CV]  fit_intercept=False, alpha=0.01, score=(train=-0.506, test=-0.504), total= 1.9min\n",
      "[CV] fit_intercept=False, alpha=10 ...................................\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  5.1min\n",
      "[CV]  fit_intercept=False, alpha=1, score=(train=-2.413, test=-2.417), total=  27.5s\n",
      "[CV] fit_intercept=False, alpha=10 ...................................\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  5.4min\n",
      "[CV]  fit_intercept=False, alpha=1, score=(train=-2.412, test=-2.414), total=  21.6s\n",
      "[CV] fit_intercept=False, alpha=100 ..................................[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:  5.4min\n",
      "\n",
      "[CV]  fit_intercept=False, alpha=0.001, score=(train=-0.391, test=-0.391), total= 5.4min\n",
      "[CV] fit_intercept=False, alpha=100 ..................................\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:  5.8min\n",
      "[CV]  fit_intercept=False, alpha=10, score=(train=-9.434, test=-9.428), total=  54.3s\n",
      "[CV] fit_intercept=False, alpha=100 ..................................[Parallel(n_jobs=-1)]: Done  13 out of  27 | elapsed:  6.0min remaining:  6.5min\n",
      "\n",
      "[CV]  fit_intercept=False, alpha=10, score=(train=-9.427, test=-9.442), total=  50.2s\n",
      "[CV] fit_intercept=False, alpha=1000 .................................[Parallel(n_jobs=-1)]: Done  14 out of  27 | elapsed:  6.1min remaining:  5.7min\n",
      "\n",
      "[CV]  fit_intercept=False, alpha=10, score=(train=-9.435, test=-9.426), total=  42.7s\n",
      "[CV] fit_intercept=False, alpha=1000 .................................[Parallel(n_jobs=-1)]: Done  15 out of  27 | elapsed:  6.2min remaining:  5.0min\n",
      "\n",
      "[CV]  fit_intercept=False, alpha=100, score=(train=-9.434, test=-9.428), total=  43.8s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  27 | elapsed:  6.2min remaining:  4.3min\n",
      "[CV] fit_intercept=False, alpha=1000 .................................\n",
      "[CV]  fit_intercept=False, alpha=100, score=(train=-9.427, test=-9.442), total=  55.4s\n",
      "[CV] fit_intercept=False, alpha=10000 ................................[Parallel(n_jobs=-1)]: Done  17 out of  27 | elapsed:  6.9min remaining:  4.1min\n",
      "\n",
      "[CV]  fit_intercept=False, alpha=1000, score=(train=-9.434, test=-9.428), total=  57.4s\n",
      "[CV] fit_intercept=False, alpha=10000 ................................[Parallel(n_jobs=-1)]: Done  18 out of  27 | elapsed:  7.2min remaining:  3.6min\n",
      "\n",
      "[CV]  fit_intercept=False, alpha=100, score=(train=-9.435, test=-9.426), total= 1.1min\n",
      "[CV] fit_intercept=False, alpha=10000 ................................[Parallel(n_jobs=-1)]: Done  19 out of  27 | elapsed:  7.2min remaining:  3.0min\n",
      "\n",
      "[CV]  fit_intercept=False, alpha=1000, score=(train=-9.427, test=-9.442), total=  56.3s\n",
      "[CV]  fit_intercept=False, alpha=1000, score=(train=-9.435, test=-9.426), total=  56.6s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  27 | elapsed:  7.2min remaining:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  27 | elapsed:  7.2min remaining:  2.1min\n",
      "[CV]  fit_intercept=False, alpha=10000, score=(train=-9.434, test=-9.428), total=  24.1s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  27 | elapsed:  7.4min remaining:  1.7min\n",
      "[CV]  fit_intercept=False, alpha=10000, score=(train=-9.435, test=-9.426), total=  11.9s\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  27 | elapsed:  7.4min remaining:  1.3min\n",
      "[CV]  fit_intercept=False, alpha=10000, score=(train=-9.427, test=-9.442), total=  14.3s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  27 | elapsed:  7.4min remaining:   55.6s\n",
      "[CV]  fit_intercept=False, alpha=0.0001, score=(train=-0.308, test=-0.306), total=10.5min\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  27 | elapsed: 10.6min remaining:   50.7s\n",
      "[CV]  fit_intercept=False, alpha=0.0001, score=(train=-0.307, test=-0.307), total=10.6min\n",
      "[CV]  fit_intercept=False, alpha=0.0001, score=(train=-0.306, test=-0.310), total=10.8min\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed: 10.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed: 10.8min finished\n"
     ]
    }
   ],
   "source": [
    "# prepare lasso regression and use randomized search for hyperparameter tuning\n",
    "# here, use parallel_backend to boost speed and avoid no end tuning process\n",
    "from sklearn.externals.joblib import parallel_backend\n",
    "parameters = {'alpha':[0.0001,0.001,0.01,0.1,1,10,100,1000,10000],\n",
    "              'fit_intercept' : [False]}\n",
    "model = Lasso()\n",
    "rs_lasso = RandomizedSearchCV(model,\n",
    "                        param_distributions = parameters,\n",
    "                        cv = 3, \n",
    "                        scoring = 'neg_mean_squared_error',\n",
    "                        verbose = 100,\n",
    "                        return_train_score = True,\n",
    "                        n_iter=10, \n",
    "                        random_state=42)\n",
    "\n",
    "with parallel_backend('threading'):\n",
    "    rs_lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by random search are: {'fit_intercept': False, 'alpha': 0.0001}\n",
      "Best score found by random search is: -0.3074479111859845\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters found by random search are:', rs_lasso.best_params_)\n",
    "print('Best score found by random search is:', rs_lasso.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and get rmsle\n",
    "lassoReg = Lasso(alpha=0.0001, fit_intercept=False )\n",
    "lassoReg.fit(X_train, y_train)\n",
    "y_pred_lasso = lassoReg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fold %02d Lasso RMSLE:  0.556302994133279\n"
     ]
    }
   ],
   "source": [
    "lasso_RMSLE = rmsle(y_test, y_pred_lasso)\n",
    "print(' Fold %02d Lasso RMSLE: ', lasso_RMSLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light GBM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] learning_rate=0.18727005942368125, max_depth=6, n_estimators=470, num_leaves=91 [CV] learning_rate=0.18727005942368125, max_depth=6, n_estimators=470, num_leaves=91 \n",
      "\n",
      "[CV] learning_rate=0.18727005942368125, max_depth=6, n_estimators=470, num_leaves=91 [CV] learning_rate=0.2993292420985183, max_depth=3, n_estimators=414, num_leaves=94 \n",
      "[CV] learning_rate=0.2993292420985183, max_depth=3, n_estimators=414, num_leaves=94 \n",
      "\n",
      "[CV] learning_rate=0.2993292420985183, max_depth=3, n_estimators=414, num_leaves=94 [CV] learning_rate=0.22962444598293358, max_depth=6, n_estimators=299, num_leaves=43 \n",
      "\n",
      "[CV] learning_rate=0.22962444598293358, max_depth=6, n_estimators=299, num_leaves=43 \n",
      "[CV]  learning_rate=0.2993292420985183, max_depth=3, n_estimators=414, num_leaves=94, score=(train=-0.266, test=-0.273), total= 9.2min\n",
      "[CV]  learning_rate=0.2993292420985183, max_depth=3, n_estimators=414, num_leaves=94, score=(train=-0.268, test=-0.270), total= 9.2min[CV]  learning_rate=0.2993292420985183, max_depth=3, n_estimators=414, num_leaves=94, score=(train=-0.268, test=-0.269), total= 9.3min\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed: 10.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] learning_rate=0.22962444598293358, max_depth=6, n_estimators=299, num_leaves=43 \n",
      "[CV] learning_rate=0.32544423647442644, max_depth=6, n_estimators=457, num_leaves=49 \n",
      "[CV] learning_rate=0.32544423647442644, max_depth=6, n_estimators=457, num_leaves=49 \n",
      "[CV]  learning_rate=0.22962444598293358, max_depth=6, n_estimators=299, num_leaves=43, score=(train=-0.249, test=-0.260), total=26.5min\n",
      "[CV] learning_rate=0.32544423647442644, max_depth=6, n_estimators=457, num_leaves=49 \n",
      "[CV]  learning_rate=0.22962444598293358, max_depth=6, n_estimators=299, num_leaves=43, score=(train=-0.250, test=-0.258), total=27.0min\n",
      "[CV] learning_rate=0.10616955533913808, max_depth=5, n_estimators=476, num_leaves=52 \n",
      "[CV]  learning_rate=0.18727005942368125, max_depth=6, n_estimators=470, num_leaves=91, score=(train=-0.242, test=-0.250), total=39.2min\n",
      "[CV]  learning_rate=0.18727005942368125, max_depth=6, n_estimators=470, num_leaves=91, score=(train=-0.240, test=-0.253), total=39.2min\n",
      "[CV] learning_rate=0.10616955533913808, max_depth=5, n_estimators=476, num_leaves=52 \n",
      "[CV]  learning_rate=0.18727005942368125, max_depth=6, n_estimators=470, num_leaves=91, score=(train=-0.243, test=-0.252), total=39.2min\n",
      "[CV] learning_rate=0.10616955533913808, max_depth=5, n_estimators=476, num_leaves=52 \n",
      "[CV] learning_rate=0.15212112147976886, max_depth=7, n_estimators=452, num_leaves=68 \n",
      "[CV]  learning_rate=0.22962444598293358, max_depth=6, n_estimators=299, num_leaves=43, score=(train=-0.251, test=-0.258), total=33.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed: 45.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] learning_rate=0.15212112147976886, max_depth=7, n_estimators=452, num_leaves=68 \n",
      "[CV]  learning_rate=0.32544423647442644, max_depth=6, n_estimators=457, num_leaves=49, score=(train=-0.225, test=-0.238), total=40.9min\n",
      "[CV]  learning_rate=0.32544423647442644, max_depth=6, n_estimators=457, num_leaves=49, score=(train=-0.224, test=-0.242), total=41.0min\n",
      "[CV] learning_rate=0.15212112147976886, max_depth=7, n_estimators=452, num_leaves=68 \n",
      "[CV] learning_rate=0.2623873301291946, max_depth=3, n_estimators=387, num_leaves=99 \n",
      "[CV]  learning_rate=0.10616955533913808, max_depth=5, n_estimators=476, num_leaves=52, score=(train=-0.271, test=-0.275), total=34.0min\n",
      "[CV] learning_rate=0.2623873301291946, max_depth=3, n_estimators=387, num_leaves=99 \n",
      "[CV]  learning_rate=0.2623873301291946, max_depth=3, n_estimators=387, num_leaves=99, score=(train=-0.275, test=-0.277), total=12.6min\n",
      "[CV] learning_rate=0.2623873301291946, max_depth=3, n_estimators=387, num_leaves=99 \n",
      "[CV]  learning_rate=0.32544423647442644, max_depth=6, n_estimators=457, num_leaves=49, score=(train=-0.227, test=-0.239), total=39.8min\n",
      "[CV] learning_rate=0.11638567021515211, max_depth=7, n_estimators=374, num_leaves=81 \n",
      "[CV]  learning_rate=0.10616955533913808, max_depth=5, n_estimators=476, num_leaves=52, score=(train=-0.273, test=-0.275), total=31.6min\n",
      "[CV] learning_rate=0.11638567021515211, max_depth=7, n_estimators=374, num_leaves=81 \n",
      "[CV]  learning_rate=0.10616955533913808, max_depth=5, n_estimators=476, num_leaves=52, score=(train=-0.271, test=-0.278), total=32.0min\n",
      "[CV] learning_rate=0.11638567021515211, max_depth=7, n_estimators=374, num_leaves=81 \n",
      "[CV]  learning_rate=0.2623873301291946, max_depth=3, n_estimators=387, num_leaves=99, score=(train=-0.274, test=-0.281), total=13.6min\n",
      "[CV] learning_rate=0.09983689107917987, max_depth=5, n_estimators=330, num_leaves=70 \n",
      "[CV]  learning_rate=0.2623873301291946, max_depth=3, n_estimators=387, num_leaves=99, score=(train=-0.276, test=-0.276), total=13.7min\n",
      "[CV] learning_rate=0.09983689107917987, max_depth=5, n_estimators=330, num_leaves=70 \n",
      "[CV]  learning_rate=0.15212112147976886, max_depth=7, n_estimators=452, num_leaves=68, score=(train=-0.241, test=-0.251), total=57.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  19 out of  30 | elapsed: 101.8min remaining: 59.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] learning_rate=0.09983689107917987, max_depth=5, n_estimators=330, num_leaves=70 \n",
      "[CV]  learning_rate=0.15212112147976886, max_depth=7, n_estimators=452, num_leaves=68, score=(train=-0.239, test=-0.253), total=59.0min\n",
      "[CV] learning_rate=0.34015376929388985, max_depth=2, n_estimators=366, num_leaves=37 \n",
      "[CV]  learning_rate=0.09983689107917987, max_depth=5, n_estimators=330, num_leaves=70, score=(train=-0.288, test=-0.291), total=30.9min\n",
      "[CV] learning_rate=0.34015376929388985, max_depth=2, n_estimators=366, num_leaves=37 \n",
      "[CV]  learning_rate=0.15212112147976886, max_depth=7, n_estimators=452, num_leaves=68, score=(train=-0.241, test=-0.250), total=57.5min\n",
      "[CV] learning_rate=0.34015376929388985, max_depth=2, n_estimators=366, num_leaves=37 \n",
      "[CV]  learning_rate=0.09983689107917987, max_depth=5, n_estimators=330, num_leaves=70, score=(train=-0.287, test=-0.294), total=31.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  23 out of  30 | elapsed: 115.6min remaining: 35.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.34015376929388985, max_depth=2, n_estimators=366, num_leaves=37, score=(train=-0.289, test=-0.290), total=10.5min\n",
      "[CV]  learning_rate=0.34015376929388985, max_depth=2, n_estimators=366, num_leaves=37, score=(train=-0.286, test=-0.292), total= 7.2min\n",
      "[CV]  learning_rate=0.11638567021515211, max_depth=7, n_estimators=374, num_leaves=81, score=(train=-0.257, test=-0.264), total=48.6min\n",
      "[CV]  learning_rate=0.34015376929388985, max_depth=2, n_estimators=366, num_leaves=37, score=(train=-0.288, test=-0.288), total= 5.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  27 out of  30 | elapsed: 120.0min remaining: 13.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.09983689107917987, max_depth=5, n_estimators=330, num_leaves=70, score=(train=-0.289, test=-0.290), total=18.7min\n",
      "[CV]  learning_rate=0.11638567021515211, max_depth=7, n_estimators=374, num_leaves=81, score=(train=-0.258, test=-0.263), total=44.3min\n",
      "[CV]  learning_rate=0.11638567021515211, max_depth=7, n_estimators=374, num_leaves=81, score=(train=-0.255, test=-0.267), total=44.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 121.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by random search are: {'learning_rate': 0.32544423647442644, 'max_depth': 6, 'n_estimators': 457, 'num_leaves': 49}\n",
      "Best score found by random search is: -0.23973342852724522\n"
     ]
    }
   ],
   "source": [
    "# prepare lightGBM regressor and use randomized search for hyperparameter tuning\n",
    "# instead of grid search, randmized search is more effective and time saving in my models\n",
    "# here, use parallel_backend to boost speed and avoid no end tuning process\n",
    "rsParams = {'learning_rate': uniform(0, 0.5),\n",
    "          'n_estimators': sp_randint(200, 500),\n",
    "          'num_leaves': sp_randint(20, 100),\n",
    "          'max_depth': sp_randint(2, 8)\n",
    "           }\n",
    "lgbm_params ={'subsample': 0.9, 'colsample_bytree': 0.8, 'min_child_samples': 50, 'objective': 'regression'\n",
    "        }\n",
    "model = LGBMRegressor(**lgbm_params)\n",
    "\n",
    "rs_lgbm = RandomizedSearchCV(model, param_distributions=rsParams, n_iter=10, cv=3, random_state=42, \n",
    "                                scoring='neg_mean_squared_error', verbose=10, return_train_score=True)\n",
    "\n",
    "with parallel_backend('threading'):\n",
    "    rs_lgbm.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameters found by random search are:', rs_lgbm.best_params_)\n",
    "print('Best score found by random search is:', rs_lgbm.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and get rmsle\n",
    "lgbm_params ={'subsample': 0.9, 'colsample_bytree': 0.8, 'min_child_samples': 50, 'objective': 'regression','boosting_type': 'gbdt','learning_rate': 0.32544423647442644,'max_depth': 6,'n_estimators': 457,'num_leaves': 49,\n",
    "      }\n",
    "lgbm_reg = LGBMRegressor(**lgbm_params)\n",
    "lgbm_reg.fit(X_train, y_train, verbose=True)\n",
    "\n",
    "y_pred_lgbm = lgbm_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fold Light GMB RMSLE:  0.48670853024263994\n"
     ]
    }
   ],
   "source": [
    "lgbm_RMSLE = rmsle(y_test, y_pred_lgbm)\n",
    "print(' Fold Light GMB RMSLE: ', lgbm_RMSLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.7 [CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.7 \n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.7 \n",
      "\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=5, learning_rate=0.07, colsample_bytree=0.7 \n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=5, learning_rate=0.07, colsample_bytree=0.7 \n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=5, learning_rate=0.07, colsample_bytree=0.7 \n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.7 \n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.7 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=5, learning_rate=0.07, colsample_bytree=0.7, score=(train=0.301, test=0.301), total=10.0min\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 10.9min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.7 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=5, learning_rate=0.07, colsample_bytree=0.7, score=(train=0.302, test=0.297), total=10.3min\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed: 11.0min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.07, colsample_bytree=0.7 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=5, learning_rate=0.07, colsample_bytree=0.7, score=(train=0.301, test=0.300), total=10.6min\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed: 11.0min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.07, colsample_bytree=0.7 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.7, score=(train=0.352, test=0.344), total=14.1min\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed: 14.9min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.07, colsample_bytree=0.7 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.7, score=(train=0.350, test=0.346), total=14.5min\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed: 15.0min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.03, colsample_bytree=0.3 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.7, score=(train=0.352, test=0.350), total=12.8min\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed: 24.4min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.03, colsample_bytree=0.3 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.03, colsample_bytree=0.3, score=(train=0.260, test=0.259), total= 9.5min\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed: 25.5min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.03, colsample_bytree=0.3 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.7, score=(train=0.416, test=0.405), total=25.5min\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed: 26.6min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=5, learning_rate=0.03, colsample_bytree=0.7 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.7, score=(train=0.415, test=0.409), total=25.7min\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed: 26.6min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=5, learning_rate=0.03, colsample_bytree=0.7 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.7, score=(train=0.416, test=0.410), total=26.0min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed: 26.7min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=5, learning_rate=0.03, colsample_bytree=0.7 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.07, colsample_bytree=0.7, score=(train=0.367, test=0.365), total=17.2min\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed: 29.1min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.05, colsample_bytree=0.7 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.07, colsample_bytree=0.7, score=(train=0.367, test=0.360), total=17.7min\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed: 29.3min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.05, colsample_bytree=0.7 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.07, colsample_bytree=0.7, score=(train=0.365, test=0.364), total=17.2min\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed: 32.6min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.05, colsample_bytree=0.7 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=5, learning_rate=0.03, colsample_bytree=0.7, score=(train=-0.293, test=-0.292), total= 9.6min\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed: 37.7min\n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=5, learning_rate=0.03, colsample_bytree=0.7, score=(train=-0.292, test=-0.297), total= 9.8min[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.05, colsample_bytree=0.3 \n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed: 37.7min\n",
      "\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.05, colsample_bytree=0.3 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=50, min_child_weight=4, max_depth=5, learning_rate=0.03, colsample_bytree=0.7, score=(train=-0.292, test=-0.291), total=10.2min\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  30 | elapsed: 37.8min remaining: 33.1min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.05, colsample_bytree=0.3 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.03, colsample_bytree=0.3, score=(train=0.260, test=0.256), total=12.8min\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  30 | elapsed: 37.8min remaining: 28.9min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.3 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.03, colsample_bytree=0.3, score=(train=0.262, test=0.262), total=11.8min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  30 | elapsed: 38.6min remaining: 25.7min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.3 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.05, colsample_bytree=0.7, score=(train=0.387, test=0.382), total=20.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  19 out of  30 | elapsed: 51.1min remaining: 29.6min\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.3 \n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.3, score=(train=0.417, test=0.411), total=13.7min\n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.05, colsample_bytree=0.3, score=(train=0.384, test=0.381), total=13.1min\n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.05, colsample_bytree=0.3, score=(train=0.385, test=0.381), total=13.6min[Parallel(n_jobs=-1)]: Done  20 out of  30 | elapsed: 52.4min remaining: 26.2min\n",
      "\n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.05, colsample_bytree=0.7, score=(train=0.390, test=0.380), total=21.9min\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  30 | elapsed: 52.4min remaining: 22.5min\n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.05, colsample_bytree=0.3, score=(train=0.388, test=0.378), total=13.6min[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.05, colsample_bytree=0.7 [Parallel(n_jobs=-1)]: Done  22 out of  30 | elapsed: 52.5min remaining: 19.1min\n",
      "\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.05, colsample_bytree=0.7 [Parallel(n_jobs=-1)]: Done  23 out of  30 | elapsed: 52.5min remaining: 16.0min\n",
      "\n",
      "[CV] subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.05, colsample_bytree=0.7 \n",
      "\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  30 | elapsed: 52.5min remaining: 13.1min\n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.3, score=(train=0.417, test=0.406), total=13.4min\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  30 | elapsed: 52.5min remaining: 10.5min\n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.05, colsample_bytree=0.7, score=(train=0.387, test=0.383), total=20.6min\n",
      "[Parallel(n_jobs=-1)]: Done  26 out of  30 | elapsed: 53.6min remaining:  8.2min\n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=7, learning_rate=0.07, colsample_bytree=0.3, score=(train=0.413, test=0.408), total= 8.2min\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  30 | elapsed: 59.8min remaining:  6.6min\n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.05, colsample_bytree=0.7, score=(train=0.338, test=0.337), total= 8.1min\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  30 | elapsed: 61.1min remaining:  4.4min\n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.05, colsample_bytree=0.7, score=(train=0.340, test=0.334), total= 8.3min\n",
      "[CV]  subsample=0.7, objective=reg:linear, n_estimators=100, min_child_weight=4, max_depth=5, learning_rate=0.05, colsample_bytree=0.7, score=(train=0.338, test=0.337), total= 8.4min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 61.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 61.1min finished\n",
      "[16:13:13] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:15:11] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Best parameters found by random search are: {'subsample': 0.7, 'objective': 'reg:linear', 'n_estimators': 100, 'min_child_weight': 4, 'max_depth': 7, 'learning_rate': 0.07, 'colsample_bytree': 0.3}\n",
      "Best score found by random search is: 0.40832756852980534\n"
     ]
    }
   ],
   "source": [
    "# prepare XGBoost regressor and use randomized search for hyperparameter tuning\n",
    "from sklearn.externals.joblib import parallel_backend\n",
    "xgbr = xgb.XGBRegressor()\n",
    "parameters = {'objective':['reg:linear'],\n",
    "              'learning_rate': [.03, 0.05, .07], #so called `eta` value\n",
    "              'max_depth': [5,7],\n",
    "              'min_child_weight': [4],\n",
    "              'subsample': [0.7],\n",
    "              'colsample_bytree': [0.3,0.7],\n",
    "              'n_estimators': [50,100]}\n",
    "rs_xgb = RandomizedSearchCV(xgbr, param_distributions=parameters, cv=3, n_iter=10,\n",
    "                            verbose=100, return_train_score=True)\n",
    "\n",
    "\n",
    "with parallel_backend('threading'):\n",
    "    rs_xgb.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "\n",
    "print('Best parameters found by random search are:', rs_xgb.best_params_)\n",
    "print('Best score found by random search is:', rs_xgb.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:07:19] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[18:09:19] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.07, max_delta_step=0, max_depth=7,\n",
       "             min_child_weight=4, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "             objective='reg:linear', random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "             scale_pos_weight=1, subsample=0.7, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit and get rmsle \n",
    "xgb_param ={'subsample': 0.7, 'objective': 'reg:linear', 'n_estimators': 100, 'min_child_weight': 4, 'max_depth': 7, 'learning_rate': 0.07, 'colsample_bytree': 0.3}\n",
    "xgb_reg = xgb.XGBRegressor(**rsreg_param)\n",
    "xgb_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5776107097698279\n"
     ]
    }
   ],
   "source": [
    "y_pred_xgb = xgb_reg.predict(X_test)\n",
    "xgb_RMSLE = rmsle(y_test, y_pred_xgb)\n",
    "print(xgb_RMSLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Modeling Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on existing methods, I would like to try several models with/without different word embedding methods with Convolutional Neural Networks to see if I can get a lower RMSLE value compared to results from machine learning models. Below I chose to use CNN because it is faster than LSTM according to existing modeling results from others, but it would be interesting to try LSTM in the future for improvements.\n",
    "<br>\n",
    "<br> In this notebook, I only show a CNN model without word embedding (tokenization only on \"name\" and \"item description\" columns). In Part 4, I will specifically show 4 ways of embeddings with CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Mostly same as what I did for machine learning preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from numpy.random import shuffle\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Activation, Flatten, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers import concatenate, BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1482535, 16), (1482535, 16))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load datasets\n",
    "train = pd.read_csv('df_train.csv')\n",
    "test = pd.read_csv('df_train.csv')\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split target variable and train dataset\n",
    "y_train = train['log_price']\n",
    "train.drop(['train_id','log_price'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing following columns:  {'negative', 'name_len', 'positive', 'neutral', 'desc_len', 'compound'}\n"
     ]
    }
   ],
   "source": [
    "# normalize numerical features using min-max normalization\n",
    "cols = set(train.columns.values)\n",
    "skip_cols = {'item_condition_id', 'brand_name',\n",
    "  'shipping', 'item_description', 'main_cat',\n",
    "  'subcat1', 'subcat2','name'}\n",
    "\n",
    "cols_to_normalize = cols - skip_cols\n",
    "print(\"Normalizing following columns: \", cols_to_normalize)\n",
    "\n",
    "def normalize(df):\n",
    "    result=df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        if (feature_name in cols_to_normalize):\n",
    "            max_value = df[feature_name].max()\n",
    "            min_value = df[feature_name].min()\n",
    "            result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result\n",
    "\n",
    "train = normalize(train)\n",
    "test = normalize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1482535/1482535 [00:02<00:00, 499977.24it/s]\n",
      "100%|██████████| 1482535/1482535 [00:03<00:00, 492239.09it/s]\n"
     ]
    }
   ],
   "source": [
    "def clean_cat(cat_values):\n",
    "    '''takes categorical column values as arguments and returns list of cleaned categories'''\n",
    "    \n",
    "    catogories = list(cat_values)\n",
    "\n",
    "    cat_list = []\n",
    "    for i in tqdm(catogories):\n",
    "        i = re.sub('[^A-Za-z0-9]+', ' ', i)\n",
    "        i = i.replace(' ','')\n",
    "        i = i.replace('&','_')\n",
    "        cat_list.append(i.strip())\n",
    "    \n",
    "    return cat_list\n",
    "\n",
    "# Cleaning brand name before using count vectorizer\n",
    "# Using same preprocessing as used earlier for categories: 'clean_cat()' function\n",
    "\n",
    "train['brand_name'] = clean_cat(train['brand_name'].values)\n",
    "test['brand_name'] = clean_cat(test['brand_name'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding categorical variables for embedding\n",
    "label = LabelEncoder()\n",
    "\n",
    "label.fit(train.brand_name) # brand name \n",
    "train.brand_name = label.transform(train.brand_name)\n",
    "\n",
    "label.fit(train.main_cat) # sub_cat0\n",
    "train.main_cat = label.transform(train.main_cat)\n",
    "\n",
    "label.fit(train.subcat1) # sub_cat_1\n",
    "train.subcat1 = label.transform(train.subcat1)\n",
    "\n",
    "label.fit(train.subcat2) # sub_cat2\n",
    "train.subcat2 = label.transform(train.subcat2)\n",
    "\n",
    "del label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, I chose to encode categorical variables using label encoder because I want to avoid high memory consumption that could lead to slow training on deep learning models. It is better to use a one-hot-encoding if possible, as the categories are not ordinal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1482535/1482535 [00:05<00:00, 285069.64it/s]\n",
      "100%|██████████| 1482535/1482535 [00:05<00:00, 279610.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# define function to preprocess \"name\"\n",
    "def preprocess_name(text_col):\n",
    "    preprocessed_names = []\n",
    "    for sentence in tqdm(text_col.values):\n",
    "        sent = sentence.replace('\\\\r', ' ')\n",
    "        sent = sent.replace('\\\\\"', ' ')\n",
    "        sent = sent.replace('\\\\n', ' ')\n",
    "        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "        preprocessed_names.append(sent.lower().strip())\n",
    "    return preprocessed_names\n",
    "\n",
    "# preprocess \"name\"\n",
    "train['name'] = preprocess_name(train['name'])\n",
    "test['name'] = preprocess_name(test['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text to Sequence Data.\n",
    "# combining columns, item_description, name, category_name.\n",
    "full_text = np.hstack([train.item_description.str.lower(), train.name.str.lower()])\n",
    "# full_text_test = np.hstack([test.item_description.str.lower(), test.name.str.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(full_text)\n",
    "train['seq_desc'] = tokenizer.texts_to_sequences(train.item_description.str.lower()) \n",
    "train['seq_name'] = tokenizer.texts_to_sequences(train.name.str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding out max len of all the text data combined\n",
    "max_len_brand = np.max(train.brand_name.max())    # brand \n",
    "max_len_condition = np.max(int(max(train.item_condition_id)))  # item_condition\n",
    "max_len_desc = np.max(int(train.desc_len.max()))  # desc_len\n",
    "max_len_name = np.max(int(train.name_len.max()))  # name_len\n",
    "max_len_sub0 = np.max(int(train.main_cat.max()))  # main_cat\n",
    "max_len_sub1 = np.max(int(train.subcat1.max()))  # Sub_1\n",
    "max_len_sub2 = np.max(train.subcat2.max())  # Sub_2\n",
    "# defining max length for padding text data\n",
    "name_padding = 15\n",
    "description_padding = 80\n",
    "max_len = np.max([np.max(train.seq_name.max()),np.max(train.seq_desc.max()),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test split\n",
    "x_tr, x_te, Y_train, Y_test = train_test_split(train,y_train, random_state=42, train_size=0.7)\n",
    "Y_train = Y_train.values.reshape(-1, 1)\n",
    "Y_test = Y_test.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN without word embeddings as weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "x_train_padded = { \n",
    "\"name\" : pad_sequences(x_tr.seq_name, maxlen= name_padding),\n",
    "\"item_desc\" : pad_sequences(x_tr.seq_desc, maxlen= description_padding),\n",
    "\"brand_name\" : np.array(x_tr.brand_name),\n",
    "\"item_condition\" : np.array(x_tr.item_condition_id),\n",
    "\"shipping\" : np.array(x_tr[[\"shipping\"]]),\n",
    "\"desc_len\" : np.array(x_tr[[\"desc_len\"]]),\n",
    "\"name_len\" : np.array(x_tr[[\"name_len\"]]),\n",
    "\"subcat_0\" : np.array(x_tr.main_cat),\n",
    "\"subcat_1\"  : np.array(x_tr.subcat1),\n",
    "\"subcat_2\" : np.array(x_tr.subcat2),\n",
    "} \n",
    "x_test_padded = {\n",
    "\"name\" : pad_sequences(x_te.seq_name, maxlen= name_padding),\n",
    "\"item_desc\" : pad_sequences(x_te.seq_desc, maxlen= description_padding),\n",
    "\"brand_name\" : np.array(x_te.brand_name),\n",
    "\"item_condition\" : np.array(x_te.item_condition_id),\n",
    "\"shipping\" : np.array(x_te[[\"shipping\"]]),\n",
    "\"desc_len\" : np.array(x_te[[\"desc_len\"]]),\n",
    "\"name_len\" : np.array(x_te[[\"name_len\"]]),\n",
    "\"subcat_0\" : np.array(x_te.main_cat),\n",
    "\"subcat_1\" : np.array(x_te.subcat1),\n",
    "\"subcat_2\" : np.array(x_te.subcat2),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = x_train_padded\n",
    "x_te = x_test_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define attributes\n",
    "batch_size = 512 * 3\n",
    "epochs = 2\n",
    "exp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n",
    "steps = int(len(x_tr['name']) / batch_size) * epochs\n",
    "lr_init, lr_fin = 0.005, 0.001\n",
    "lr_decay = exp_decay(lr_init, lr_fin, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definign inputs\n",
    "subcat_0 = Input(shape=[1], name=\"subcat_0\")\n",
    "subcat_1 = Input(shape=[1], name=\"subcat_1\")\n",
    "subcat_2 = Input(shape=[1], name=\"subcat_2\")\n",
    "desc_len = Input(shape=[1], name=\"desc_len\")\n",
    "name_len = Input(shape=[1], name=\"name_len\")\n",
    "\n",
    "brand_name = Input(shape=[1], name=\"brand_name\")\n",
    "\n",
    "num_vars = Input(shape=[1], name=\"shipping\")\n",
    "\n",
    "item_condition = Input(shape=[1], name=\"item_condition\")\n",
    "\n",
    "name = Input(shape=[x_tr[\"name\"].shape[1]], name=\"name\") # 15 shape = [15]\n",
    "item_desc = Input(shape=[x_tr[\"item_desc\"].shape[1]], name=\"item_desc\") # 80 shape = [80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define embedding layers, max lengthes raised larger becuase of the system error using tensorflow 2.x\n",
    "sub0_emb = Embedding(max_len_sub0+10, 10)(subcat_0)\n",
    "sub1_emb = Embedding(max_len_sub1+10, 10)(subcat_1)\n",
    "sub2_emb = Embedding(max_len_sub2+10, 10)(subcat_2)\n",
    "\n",
    "brand_emb = Embedding(max_len_brand+10, 10)(brand_name)\n",
    "\n",
    "item_cond_emb = Embedding(max_len_condition+10, 5)(item_condition)\n",
    "\n",
    "name_emb = Embedding(max_len+10, 15, weights=[embedding_matrix])(name)\n",
    "item_desc_emb = Embedding(max_len+10, 80, weights=[embedding_matrix])(item_desc)\n",
    "\n",
    "desc_len_emb = Embedding(max_len_desc+10, 5)(desc_len)\n",
    "name_len_emb = Embedding(max_len_name+10, 5)(name_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn and pooling layers\n",
    "convs1 = []\n",
    "convs2 = []\n",
    "for filter_length in [1,2]:\n",
    "  cnn_layer1 = Conv1D(filters=50, kernel_size=filter_length, padding='same', activation='relu', strides=1) (name_emb)\n",
    "  cnn_layer2 = Conv1D(filters=50, kernel_size=filter_length, padding='same', activation='relu', strides=1) (item_desc_emb)\n",
    "  maxpool1 = GlobalMaxPooling1D() (cnn_layer1)\n",
    "  maxpool2 = GlobalMaxPooling1D() (cnn_layer2)\n",
    "  convs1.append(maxpool1)\n",
    "  convs2.append(maxpool2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs1 = concatenate(convs1)\n",
    "convs2 = concatenate(convs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten\n",
    "flat_1 = Flatten() (brand_emb)\n",
    "flat_2 = Flatten() (item_cond_emb)\n",
    "flat_5 = Flatten() (sub0_emb)\n",
    "flat_6 = Flatten() (sub1_emb)\n",
    "flat_7 = Flatten() (sub2_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate for further steps\n",
    "main_l = concatenate([flat_1,flat_2,flat_5,flat_6,flat_7,embed_1,convs1,convs2, num_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dropout and batchnormalization layers\n",
    "main_l = Dropout(0.1)(Dense(256,kernel_initializer='normal',activation='relu') (main_l))\n",
    "main_l = BatchNormalization()(Dense(128,kernel_initializer='normal',activation='relu') (main_l))\n",
    "main_l = Dropout(0.1)(Dense(64,kernel_initializer='normal',activation='relu') (main_l))\n",
    "main_l = BatchNormalization()(Dense(32,kernel_initializer='normal',activation='relu') (main_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_23\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "name (InputLayer)               [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_desc (InputLayer)          [(None, 80)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_104 (Embedding)       (None, 15, 15)       3749190     name[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_105 (Embedding)       (None, 80, 80)       19995680    item_desc[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "brand_name (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_condition (InputLayer)     [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "subcat_0 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "subcat_1 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "subcat_2 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 15, 50)       800         embedding_104[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 15, 50)       1550        embedding_104[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 80, 50)       4050        embedding_105[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 80, 50)       8050        embedding_105[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_102 (Embedding)       (None, 1, 10)        48100       brand_name[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_103 (Embedding)       (None, 1, 5)         75          item_condition[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_99 (Embedding)        (None, 1, 10)        190         subcat_0[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_100 (Embedding)       (None, 1, 10)        1220        subcat_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_101 (Embedding)       (None, 1, 10)        8790        subcat_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_44 (Global (None, 50)           0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_46 (Global (None, 50)           0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_45 (Global (None, 50)           0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_47 (Global (None, 50)           0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_55 (Flatten)            (None, 10)           0           embedding_102[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_56 (Flatten)            (None, 5)            0           embedding_103[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_57 (Flatten)            (None, 10)           0           embedding_99[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_58 (Flatten)            (None, 10)           0           embedding_100[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_59 (Flatten)            (None, 10)           0           embedding_101[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 100)          0           global_max_pooling1d_44[0][0]    \n",
      "                                                                 global_max_pooling1d_46[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 100)          0           global_max_pooling1d_45[0][0]    \n",
      "                                                                 global_max_pooling1d_47[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "shipping (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 246)          0           flatten_55[0][0]                 \n",
      "                                                                 flatten_56[0][0]                 \n",
      "                                                                 flatten_57[0][0]                 \n",
      "                                                                 flatten_58[0][0]                 \n",
      "                                                                 flatten_59[0][0]                 \n",
      "                                                                 concatenate_33[0][0]             \n",
      "                                                                 concatenate_34[0][0]             \n",
      "                                                                 shipping[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_56 (Dense)                (None, 256)          63232       concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 256)          0           dense_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_57 (Dense)                (None, 128)          32896       dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 128)          512         dense_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_58 (Dense)                (None, 64)           8256        batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 64)           0           dense_58[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_59 (Dense)                (None, 32)           2080        dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 32)           128         dense_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "desc_len (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "name_len (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_60 (Dense)                (None, 1)            33          batch_normalization_23[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 23,924,832\n",
      "Trainable params: 23,924,512\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define output layer and optimizer\n",
    "output = Dense(1, activation=\"linear\") (main_l)\n",
    "model_2 = Model([name, item_desc, brand_name , desc_len,name_len, item_condition, subcat_0, subcat_1, subcat_2, num_vars ], output)\n",
    "optimizer = optimizers.Adam(lr = 0.005)\n",
    "# compile the model\n",
    "model_2.compile(loss = 'mse', optimizer = optimizer)\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "676/676 [==============================] - 260s 385ms/step - loss: 0.5104\n",
      "Epoch 2/2\n",
      "676/676 [==============================] - 248s 367ms/step - loss: 0.1813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fee9ad5e290>"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit cnn model\n",
    "model_2.fit(x_tr, Y_train, epochs= 2, batch_size= 512 * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RMSLE error: 0.45484033271260815\n"
     ]
    }
   ],
   "source": [
    "# get the error\n",
    "y_pred = model_2.predict(x_te, batch_size=batch_size)\n",
    "print(\"RMSLE error:\", rmsle(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
